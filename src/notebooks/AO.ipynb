{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908ee260-c467-4fbd-979b-b47736ad71fd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #F0F0F0; padding: 10px; font-size: 32px; font-weight: bold; text-align: center; border-radius: 10px;\">\n",
    "  <span style=\"color: green;\">S423 - Automatisk Oppretninger ved hjelp av Machine Learning</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5f510-1a64-413e-a12a-50877ed23b46",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Setup and Configuration](#Setup-and-Configuration)\n",
    "3. [Data Loading and Preprocessing](#Data-Loading-and-Preprocessing)\n",
    "4. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "5. [Model Building](#Model-Building)\n",
    "6. [Evaluation](#Evaluation)\n",
    "7. [Conclusion](#Conclusion)\n",
    "8. [References](#References)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5c785d-fbf2-4d96-973f-b954e8ebc111",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536c8b7d-dbcb-462c-b6f1-eb2ec20ecedc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gcsfs\n",
    "import getpass\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import geopandas as gpd\n",
    "import sgis as sg\n",
    "import dapla as dp\n",
    "import datetime\n",
    "from dapla.auth import AuthClient\n",
    "from dapla import FileClient\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68416850-3f81-4f6c-81ba-3d094af63b33",
   "metadata": {},
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d481f34-d486-4126-801f-8ef6fdd5ad1f",
   "metadata": {},
   "source": [
    "This step requires the filtering of dataframes so that the correct nærings are included. All bedfrifter will be included in analysis, not just those belonging to reg_type 2. Also, and very importantly, if running for an entire 'delreg' (or an equivalent) then a backup file needs to be saved. \n",
    "\n",
    "We will also need to establish what category the foretak that is being automatically corrected belongs to. There will be three cateogories:\n",
    "\n",
    "- Good quality data. This will require a simple function to correct. It will resemble what occurred with '999'  in the old method. \n",
    "\n",
    "- Bad quality data. Here we will need to use K means in order to find nearest neighbors. This can be shown visually for fun. After finding the nearest neighbors can do one of a few things which will be discussed in the functions section. \n",
    "\n",
    "- No data. Here we will also need to find nearest neighbors and then use this to inform the function that is used. \n",
    "\n",
    "Constraints? :\n",
    "\n",
    "- totals for foretak\n",
    "- reg_type 03 / 04?\n",
    "- ikke i drift?\n",
    "- totals for salgsint / forbruk ? How many non varehandel bedrift?\n",
    "- relationship between lønn, varekostnader, driftkostnader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201e74e2-b905-45ea-b816-847749848d91",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Example foretak:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9da4eee-553d-42e3-8fd1-b6daf6a88030",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Good data\n",
    "Orgnr_f : 879263662\n",
    "##### Bad Data (missing info on one or more units)\n",
    "Orgnr_f : 934531345\n",
    "##### No Data\n",
    "Orgnr_f : "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a27dbf-aca2-452c-8211-5ef55d6629bf",
   "metadata": {},
   "source": [
    "# Basic function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66524836-f4b8-4293-8dd7-615e312f66dd",
   "metadata": {},
   "source": [
    "Were basically talking about a constrained optimisation problem. We need to investigate several functions and determine which one is better. It may in fact be that some functions work better depending on the situtation - ideally an algorythm would determine which one is better - but for fun perhaps we can have a toggle option so users can look through which one they think works best. \n",
    "\n",
    "Options for functions might include: \n",
    "\n",
    "- using neighest neighbors and their averages in a function to calculate a bedrifters sales. After this "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f99d066-6514-4694-a039-3b9122e8a650",
   "metadata": {},
   "source": [
    "Constrained optimization in the context of machine learning typically involves defining an objective function that you want to minimize or maximize while keeping certain constraints satisfied. In your case, the constraint is that the sum of all estimated revenues for 'bedrifter' under a single 'foretak' must equal the known total revenue for that 'foretak'.\n",
    "\n",
    "Here's how you might proceed with a constrained optimization approach to predict the percentage distribution of revenues:\n",
    "\n",
    "1. **Normalize the Data**: Transform the revenue of each 'bedrift' into a proportion of the total revenue of its parent 'foretak'. This is your target variable for modeling.\n",
    "\n",
    "2. **Feature Selection**: Choose features that could influence the revenue distribution among 'bedrifter'. For instance, the number of employees, the industry sector, and historical sales data.\n",
    "\n",
    "3. **Model Selection**: Select a suitable regression model. Given that you have a constraint that needs to be satisfied, you may want to look into models that can naturally handle constraints, like a linear regression model with non-negativity constraints or more advanced methods like quadratic programming.\n",
    "\n",
    "4. **Loss Function**: Define a loss function for the model training. This is what the model will minimize. A common choice is Mean Squared Error (MSE) for regression problems.\n",
    "\n",
    "5. **Constraint Handling**: To ensure that the predicted proportions for each 'foretak' sum to 1 (or 100%), you can incorporate a post-processing step that proportionally scales the predictions. Alternatively, you can build this constraint directly into the model training using a custom optimization algorithm.\n",
    "\n",
    "6. **Model Training**: Train the model on your data, minimizing the loss function while ensuring that the constraints are satisfied.\n",
    "\n",
    "7. **Model Evaluation**: Evaluate the model using a validation set or through cross-validation to ensure it generalizes well to unseen data.\n",
    "\n",
    "8. **Prediction and Rescaling**: Use the model to predict the revenue proportions for all 'bedrifter'. Rescale the predictions so that they sum to 1 for each 'foretak'.\n",
    "\n",
    "Here's an example using Python's `scipy.optimize` module, which allows for constrained optimization:\n",
    "\n",
    "```python\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Assume X_train contains the features for your bedrifter\n",
    "# and y_train contains the normalized revenue (as a percentage of the foretak's total revenue).\n",
    "\n",
    "# Define the objective function (e.g., mean squared error)\n",
    "def objective_function(coefs, X, y):\n",
    "    predictions = X.dot(coefs)\n",
    "    return ((predictions - y) ** 2).mean()\n",
    "\n",
    "# Define the constraint function (sum of predictions must equal 1 for each foretak)\n",
    "def constraint_function(coefs, X):\n",
    "    return X.dot(coefs).sum() - 1\n",
    "\n",
    "# The constraint in a form scipy can use\n",
    "constraint = {'type': 'eq', 'fun': constraint_function, 'args': (X_train,)}\n",
    "\n",
    "# Initial guess for the coefficients\n",
    "initial_coefs = np.zeros(X_train.shape[1])\n",
    "\n",
    "# Perform the optimization\n",
    "result = minimize(\n",
    "    fun=objective_function,\n",
    "    x0=initial_coefs,\n",
    "    args=(X_train, y_train),\n",
    "    constraints=constraint,\n",
    "    method='SLSQP',  # Sequential Least Squares Programming\n",
    "    options={'disp': True}\n",
    ")\n",
    "\n",
    "# The optimal coefficients are in result.x\n",
    "optimal_coefs = result.x\n",
    "\n",
    "# Predict the revenue percentages\n",
    "y_pred = X_train.dot(optimal_coefs)\n",
    "```\n",
    "\n",
    "In this example, `minimize` is used to find the coefficients of your features that minimize the mean squared error of your predictions, subject to the constraint that the predictions sum to 1. The `'SLSQP'` method supports both equality and inequality constraints.\n",
    "\n",
    "This is a simplified example and assumes that all 'bedrifter' in your dataset belong to a single 'foretak'. If you have multiple 'foretak', you would need to modify your constraint function to ensure that the sum of the predictions for each group of 'bedrifter' belonging to the same 'foretak' equals the total revenue of that 'foretak'.\n",
    "\n",
    "In practice, you may have to employ more sophisticated techniques to handle multiple constraints (one for each 'foretak'). You might need to run the optimization separately for each 'foretak' or use more complex optimization methods that can handle group-wise constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723fa098-972f-4db2-9fd7-321fd70016f7",
   "metadata": {},
   "source": [
    "# Choosing the right function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24283a84-da34-49f8-ba80-7d61637f3a0c",
   "metadata": {},
   "source": [
    "We can build several ourselves and test them directly. Or we can use machine learning to detmerine what the correct one is. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeca3e6-0ace-406b-ab1f-8f1eb451bdb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Our own functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081a6c4-5c02-4cdb-b7fe-4ed04bdb5bf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Possible contraints:\n",
    "\n",
    "- contrained by foretak totals\n",
    "- NO-poster - can we use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6cd580-98d1-40f4-a48b-69fd7a51d972",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Machine learning function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb276a99-9458-46fe-ba98-9ce4a8d52e6d",
   "metadata": {},
   "source": [
    "If the optimal function varies by industry and the goal is to minimize error across different segments such as 'bedrifter' under various 'foretak' and industries, then one approach is to use a machine learning model that can learn the function from the data. This model would be trained to minimize the prediction error and could adapt to different industries by learning from industry-specific patterns in the training data.\n",
    "\n",
    "Here are some steps to build such a model:\n",
    "\n",
    "### Step 1: Feature Engineering\n",
    "Include industry as a categorical feature in your dataset. If you have other features that vary by industry and could influence revenue distribution, include those as well.\n",
    "\n",
    "### Step 2: Model Selection\n",
    "Choose a flexible machine learning model that can capture complex relationships in the data. Models like Random Forests, Gradient Boosting Machines (like XGBoost, LightGBM), or Neural Networks can automatically learn to adjust their predictions based on the input features, which would include the industry category.\n",
    "\n",
    "### Step 3: Model Training\n",
    "Train your model on the training data, using a loss function that represents the error you want to minimize (typically Mean Squared Error for regression tasks). The model should learn to make different predictions for 'bedrifter' in different industries, as it will use the industry feature to adjust its internal decision rules.\n",
    "\n",
    "### Step 4: Cross-Validation\n",
    "Use cross-validation to evaluate the model's performance across different subsets of your data. This helps ensure that your model not only minimizes error on the training data but also generalizes well to new, unseen data.\n",
    "\n",
    "### Step 5: Constrained Optimization Post-Processing\n",
    "After you've trained your model and made predictions, you might find that the predicted revenues for 'bedrifter' under a single 'foretak' don't sum up to the total known revenue for that 'foretak'. You can then apply a constrained optimization or scaling post-processing step to adjust the predictions so that they meet this constraint.\n",
    "\n",
    "For instance, if the predicted revenues for the 'bedrifter' under one 'foretak' sum up to 90% of the known total revenue for that 'foretak', you would scale up each 'bedrift's predicted revenue by the same factor so that they sum to the total.\n",
    "\n",
    "### Step 6: Implementation\n",
    "Here's a very high-level pseudocode for implementing the above strategy using a gradient boosting machine like XGBoost:\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Prepare your data\n",
    "# X = features including industry, location, etc.\n",
    "# y = normalized revenue targets (proportions of total 'foretak' revenue)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Rescale predictions so that they sum to the total 'foretak' revenue\n",
    "# This would be done for each 'foretak' group within your dataset\n",
    "# For simplicity, let's assume 'total_revenue_by_foretak' is a dictionary\n",
    "# mapping each 'foretak' to its total revenue\n",
    "for foretak, total_revenue in total_revenue_by_foretak.items():\n",
    "    # Select the predictions for the current 'foretak'\n",
    "    pred_indices = X_test['foretak'] == foretak\n",
    "    foretak_preds = y_pred[pred_indices]\n",
    "    \n",
    "    # Calculate the scaling factor\n",
    "    scaling_factor = total_revenue / sum(foretak_preds)\n",
    "    \n",
    "    # Scale the predictions\n",
    "    y_pred[pred_indices] *= scaling_factor\n",
    "\n",
    "# Calculate the final mean squared error after rescaling\n",
    "final_mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Final Mean Squared Error: {final_mse}\")\n",
    "```\n",
    "\n",
    "This pseudocode outlines the training and prediction steps and includes a simple rescaling procedure to satisfy the revenue constraints. The key is to have 'foretak' as an identifier in your features so that you can group predictions accordingly. \n",
    "\n",
    "The actual implementation will be more complex, especially the rescaling part, as it needs to handle grouping and apply the scaling correctly for each group. The exact details would depend on your dataset's structure and the business logic that determines how 'bedrifter' are grouped under 'foretak'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f40653a-0b06-4be9-860e-c4f4e1b4cfe2",
   "metadata": {},
   "source": [
    "# Get user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3fce29-be64-4a8c-b076-6ac11e769261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ask the user for the year\n",
    "# 879263662\n",
    "\n",
    "year = input(\"Please enter the year: \")\n",
    "\n",
    "enhet = input(\"Please enter the orgnr_foretak #: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fe5228-3536-4475-bd63-ffb635f9d71e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Read in geographical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ac6973-7a23-4782-88e9-f75cc31dc4c1",
   "metadata": {},
   "source": [
    "### Current year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654e3a18-a996-46f3-bc95-ac96d518c46d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now()\n",
    "\n",
    "# Format the year and month\n",
    "current_year = current_date.strftime(\"%Y\")\n",
    "current_month = current_date.strftime(\"%m\")\n",
    "\n",
    "# Subtract one day from the first day of the current month to get the last day of the previous month\n",
    "last_day_of_previous_month = datetime.datetime(\n",
    "    current_date.year, current_date.month, 1\n",
    ") - datetime.timedelta(days=1)\n",
    "\n",
    "# Now we can get the month number of the previous month\n",
    "previous_month = last_day_of_previous_month.strftime(\"%m\")\n",
    "\n",
    "VOFSTI = (\n",
    "    \"ssb-prod-vof-data-delt/stedfesting-situasjonsuttak_data/klargjorte-data/parquet\"\n",
    ")\n",
    "file_path = (\n",
    "    f\"{VOFSTI}/stedfesting-situasjonsuttak_p{current_year}-{previous_month}_v1.parquet\"\n",
    ")\n",
    "\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aaf622-3f49-4dcb-9aaa-94d10b1e15ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vof_df = dp.read_pandas(f\"{file_path}\")\n",
    "vof_gdf = gpd.GeoDataFrame(\n",
    "    vof_df,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        vof_df[\"y_koordinat\"],\n",
    "        vof_df[\"x_koordinat\"],\n",
    "    ),\n",
    "    crs=25833,\n",
    ")\n",
    "\n",
    "vof_gdf = vof_gdf.rename(\n",
    "    columns={\n",
    "        \"orgnrbed\": \"orgnr_bedrift\",\n",
    "        \"org_nr\": \"orgnr_foretak\",\n",
    "        \"nace1_sn07\": \"naring\",\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "vof_gdf = vof_gdf[\n",
    "    [\n",
    "        \"orgnr_bedrift\",\n",
    "        \"orgnr_foretak\",\n",
    "        \"naring\",\n",
    "        \"x_koordinat\",\n",
    "        \"y_koordinat\",\n",
    "        \"rute_100m\",\n",
    "        \"rute_1000m\",\n",
    "        \"geometry\",\n",
    "    ]\n",
    "]\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "vof_gdf = vof_gdf.dropna(subset=[\"x_koordinat\"])\n",
    "vof_gdf = vof_gdf.drop_duplicates(subset=\"orgnr_bedrift\")\n",
    "vof_gdf = vof_gdf.drop(\"orgnr_foretak\", axis=1)\n",
    "vof_gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506ae70-562e-40ed-a383-b39ad1e40d33",
   "metadata": {},
   "source": [
    "# Get bedrift data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699cea3-484a-446b-aece-ae7c92c572e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fil_path = f\"gs://ssb-prod-noeku-data-produkt/statistikkfiler/g{year}/statistikkfil_bedrifter_pub.parquet\"\n",
    "\n",
    "bedrifter = pd.read_parquet(fil_path, filesystem=fs)\n",
    "\n",
    "# Create 'nace4' by slicing the first 5 characters of 'naring'\n",
    "bedrifter[\"naring4\"] = bedrifter[\"naring\"].str[:5]\n",
    "\n",
    "bedrifter[\"naring_f_4\"] = bedrifter[\"naring_f\"].str[:5]\n",
    "\n",
    "# Create 'nace3' by slicing the first 4 characters of 'naring'\n",
    "bedrifter[\"naring3\"] = bedrifter[\"naring\"].str[:4]\n",
    "\n",
    "bedrifter[\"naring_f_3\"] = bedrifter[\"naring_f\"].str[:4]\n",
    "\n",
    "enhets_id = bedrifter.loc[bedrifter[\"orgnr_foretak\"] == enhet, \"enhets_id\"].values[0]\n",
    "\n",
    "\n",
    "# Now 'bedrifter' has two new columns: 'nace4' and 'nace3'\n",
    "\n",
    "\n",
    "# bedrifter.head()\n",
    "\n",
    "# Assuming 'bedrifter' is your DataFrame, 'enhet' is your variable with the stored value\n",
    "\n",
    "# Create the 'naring3' list for rows where 'orgnr_foretak' equals 'enhet'\n",
    "naring3_list = list(\n",
    "    bedrifter.loc[bedrifter[\"orgnr_foretak\"] == enhet, \"naring3\"].unique()\n",
    ")\n",
    "\n",
    "# Create the 'naring_f_3' list for rows where 'orgnr_f' equals 'enhet'\n",
    "\n",
    "naring_f_3_list = list(\n",
    "    bedrifter.loc[bedrifter[\"orgnr_foretak\"] == enhet, \"naring_f_3\"].unique()\n",
    ")\n",
    "# Now you have two lists: 'naring3_list' and 'naring_f_3_list'\n",
    "\n",
    "\n",
    "# Assuming 'naring3_list' and 'naring_f_3_list' have been defined as per your previous instructions\n",
    "\n",
    "# Filter the DataFrame\n",
    "filtered_bedrifter = bedrifter[\n",
    "    bedrifter[\"naring3\"].isin(naring3_list)\n",
    "    | bedrifter[\"naring_f_3\"].isin(naring_f_3_list)\n",
    "]\n",
    "\n",
    "filtered_bedrifter = filtered_bedrifter[\n",
    "    [\n",
    "        \"orgnr_bedrift\",\n",
    "        \"orgnr_foretak\",\n",
    "        \"omsetning\",\n",
    "        \"kommune\",\n",
    "        \"naring_f\",\n",
    "        \"naring4\",\n",
    "        \"naring_f_4\",\n",
    "        \"naring3\",\n",
    "        \"naring_f_3\",\n",
    "        \"nopost_driftskostnader\",\n",
    "        \"sysselsetting_syss\",\n",
    "        \"navn\",\n",
    "        \"reg_type\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# Calculate the counts for each 'orgnr_foretak' and create a new column 'bedrift_count'\n",
    "filtered_bedrifter[\"bedrift_count\"] = filtered_bedrifter.groupby(\"orgnr_foretak\")[\n",
    "    \"orgnr_foretak\"\n",
    "].transform(\"count\")\n",
    "\n",
    "# Now 'filtered_bedrifter' contains only the rows that meet the conditions\n",
    "filtered_bedrifter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18711fde-05df-44c4-a200-1caae790e61e",
   "metadata": {},
   "source": [
    "# Get foretak data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814df51-79ca-4f65-a4ec-f88de6a80748",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fil_path = f\"gs://ssb-prod-noeku-data-produkt/statistikkfiler/g{year}/statistikkfil_foretak_pub.parquet\"\n",
    "\n",
    "foretak = pd.read_parquet(fil_path, filesystem=fs)\n",
    "\n",
    "foretak = foretak[\n",
    "    [\"orgnr_foretak\", \"omsetning\", \"sysselsetting_syss\", \"nopost_driftskostnader\"]\n",
    "]\n",
    "\n",
    "foretak = foretak.rename(\n",
    "    columns={\n",
    "        \"omsetning\": \"omsetning_foretak\",\n",
    "        \"sysselsetting_syss\": \"sysselsetting_foretak\",\n",
    "        \"nopost_driftskostnader\": \"nopost_driftskostnader_foretak\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Need to do this in order to calculate other variables. Not the true syss count.\n",
    "foretak[\"sysselsetting_foretak\"].fillna(1, inplace=True)\n",
    "foretak[\"sysselsetting_foretak\"].replace(0, 1, inplace=True)  # Replace 0 with 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6bee8d-0844-42b8-8838-6b698c2006fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Merge foretak and bedrifter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6efdd-6970-4060-8fc1-fd4ea619f383",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_bedrifter = filtered_bedrifter.merge(foretak, on=\"orgnr_foretak\", how=\"left\")\n",
    "filtered_bedrifter = filtered_bedrifter.drop_duplicates(\n",
    "    subset=\"orgnr_bedrift\", keep=\"first\"\n",
    ")\n",
    "\n",
    "# filtered_bedrifter = filtered_bedrifter.drop(\"orgnr_foretak\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edace7b-63ba-4acf-95ed-21cde14ef07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_bedrifter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bb9fe0-8a16-4849-b526-6ce13045c297",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ensure that there are no zero values in the denominators to avoid division by zero errors\n",
    "filtered_bedrifter[\"sysselsetting_foretak\"].replace(0, np.nan, inplace=True)\n",
    "filtered_bedrifter[\"bedrift_count\"].replace(0, np.nan, inplace=True)\n",
    "\n",
    "# Create new columns with the calculated values\n",
    "filtered_bedrifter[\"oms_per_syss_foretak\"] = (\n",
    "    filtered_bedrifter[\"omsetning_foretak\"]\n",
    "    / filtered_bedrifter[\"sysselsetting_foretak\"]\n",
    ")\n",
    "filtered_bedrifter[\"oms_per_bedrift_foretak\"] = (\n",
    "    filtered_bedrifter[\"omsetning_foretak\"] / filtered_bedrifter[\"bedrift_count\"]\n",
    ")\n",
    "\n",
    "# Display the DataFrame to verify the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27075fd-ad1f-4786-a4af-bff31589e32f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtered_bedrifter.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afded97e-7360-4af3-9334-9a0e6b2b38a3",
   "metadata": {},
   "source": [
    "# Explore the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc12b77-3141-486d-b072-479da12f42ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vof_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1454e63-6da0-4042-858b-556533391812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = filtered_bedrifter.merge(vof_gdf, on=\"orgnr_bedrift\", how=\"left\")\n",
    "merged_gdf = gpd.GeoDataFrame(merged_df, geometry=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216cb6fb-ce4e-41f5-bb94-71049e0b30ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = merged_gdf[merged_gdf[\"orgnr_foretak\"] == enhet]\n",
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ededa527-22ce-4435-8363-3135d6c554a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f2f99c-3e4b-47ba-b679-ab1bfbfb8785",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merged_gdf = merged_gdf.dropna(subset=[\"x_koordinat\", \"y_koordinat\"])\n",
    "# sg.explore(merged_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39397bf7-18d2-4838-8359-826b69f6dea5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d53d6bc-4519-497e-b7a7-0f6a7be39a31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"x_koordinat\",\n",
    "    \"y_koordinat\",\n",
    "    \"sysselsetting_syss\",\n",
    "    \"naring\",\n",
    "    \"omsetning_foretak\",\n",
    "    \"nopost_driftskostnader_foretak\",\n",
    "    \"nopost_driftskostnader_foretak\",\n",
    "    \"oms_per_syss_foretak\",\n",
    "    \"oms_per_bedrift_foretak\",\n",
    "]\n",
    "\n",
    "non_feature_columns = [\"orgnr_foretak\", \"orgnr_bedrift\"]  # Non-feature columns to keep\n",
    "\n",
    "merged_gdf = merged_gdf.dropna(subset=features + non_feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584479c7-aae4-44b3-bb9e-ecd31dc3d7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Assume 'merged_gdf' is your DataFrame and it has been preprocessed to include the following columns:\n",
    "# 'x_koordinat', 'y_koordinat', 'sysselsetting_syss', 'naring', 'omsetning'\n",
    "\n",
    "# Preprocessing\n",
    "# StandardScaler will scale the coordinates and number of employees\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            [\n",
    "                \"x_koordinat\",\n",
    "                \"y_koordinat\",\n",
    "                \"sysselsetting_syss\",\n",
    "                \"omsetning_foretak\",\n",
    "                \"nopost_driftskostnader_foretak\",\n",
    "                \"oms_per_syss_foretak\",\n",
    "                \"oms_per_bedrift_foretak\",\n",
    "            ],\n",
    "        ),\n",
    "        (\"cat\", OneHotEncoder(), [\"naring\"]),  # One-hot encode 'naring'\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model setup\n",
    "knn_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn\", KNeighborsRegressor(n_neighbors=5)),  # Adjust n_neighbors as needed\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "features = [\n",
    "    \"x_koordinat\",\n",
    "    \"y_koordinat\",\n",
    "    \"sysselsetting_syss\",\n",
    "    \"naring\",\n",
    "    \"omsetning_foretak\",\n",
    "    \"nopost_driftskostnader_foretak\",\n",
    "    \"oms_per_syss_foretak\",\n",
    "    \"oms_per_bedrift_foretak\",\n",
    "]\n",
    "target = \"omsetning\"\n",
    "\n",
    "knn_pipeline.fit(merged_gdf[features], merged_gdf[target])\n",
    "\n",
    "# To predict, you would follow a similar approach to the previous examples,\n",
    "# ensuring that you pass the x and y coordinates directly to the model after scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77188643-d6cf-4e81-a589-a9f2cf26a279",
   "metadata": {},
   "source": [
    "# Train the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b699eed8-0069-4b8d-8108-52634c321ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# # Replace 'target_variable_name' with the actual name of your target variable\n",
    "# features = [\n",
    "#     \"x_koordinat\",\n",
    "#     \"y_koordinat\",\n",
    "#     # \"sysselsetting_syss\",\n",
    "#     \"naring\",\n",
    "#     \"omsetning_foretak\",\n",
    "#     \"nopost_driftskostnader_foretak\",\n",
    "#     \"oms_per_syss_foretak\",\n",
    "#     \"oms_per_bedrift_foretak\",\n",
    "# ]\n",
    "# target = \"omsetning\"\n",
    "\n",
    "# # Preprocessing\n",
    "# # Define the preprocessing for numerical and categorical features\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\n",
    "#             \"num\",\n",
    "#             StandardScaler(),\n",
    "#             [\n",
    "#                 \"x_koordinat\",\n",
    "#                 \"y_koordinat\",\n",
    "#                 # \"sysselsetting_syss\",\n",
    "#                 \"naring\",\n",
    "#                 \"omsetning_foretak\",\n",
    "#                 \"nopost_driftskostnader_foretak\",\n",
    "#                 \"oms_per_syss_foretak\",\n",
    "#                 \"oms_per_bedrift_foretak\",\n",
    "#             ],\n",
    "#         ),\n",
    "#         (\n",
    "#             \"cat\",\n",
    "#             OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "#             [\"naring\"],\n",
    "#         ),  # Set handle_unknown to 'ignore'\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "# # Model setup\n",
    "# # Setup the pipeline with the preprocessing and the KNN model\n",
    "# knn_pipeline = Pipeline(\n",
    "#     steps=[\n",
    "#         (\"preprocessor\", preprocessor),\n",
    "#         (\"knn\", KNeighborsRegressor(n_neighbors=4)),  # Adjust n_neighbors as needed\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     merged_gdf[features + ['orgnr_bedrift', 'reg_type', 'orgnr_foretak']],\n",
    "#     merged_gdf[target],\n",
    "#     test_size=0.2,  # 20% for testing\n",
    "#     random_state=42,  # Seed for reproducibility\n",
    "# )\n",
    "\n",
    "# # Train the model\n",
    "# knn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = knn_pipeline.predict(X_test)\n",
    "\n",
    "# # Evaluate the model\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# # Calculate Mean Absolute Error (MAE)\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# # Calculate Root Mean Squared Error (RMSE)\n",
    "# rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# # Calculate R^2 Score\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# mse, mae, rmse, r2\n",
    "\n",
    "\n",
    "#########################################\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Replace 'target_variable_name' with the actual name of your target variable\n",
    "features = [\n",
    "    \"x_koordinat\",\n",
    "    \"y_koordinat\",\n",
    "    \"sysselsetting_syss\",\n",
    "    \"naring\",\n",
    "    \"omsetning_foretak\",\n",
    "    \"nopost_driftskostnader_foretak\",\n",
    "    \"oms_per_syss_foretak\",\n",
    "    \"oms_per_bedrift_foretak\",\n",
    "]\n",
    "target = \"omsetning\"\n",
    "\n",
    "# Preprocessing\n",
    "# Define the preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            [\n",
    "                \"x_koordinat\",\n",
    "                \"y_koordinat\",\n",
    "                \"sysselsetting_syss\",\n",
    "                \"omsetning_foretak\",\n",
    "                \"nopost_driftskostnader_foretak\",\n",
    "                \"oms_per_syss_foretak\",\n",
    "                \"oms_per_bedrift_foretak\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            [\"naring\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model setup\n",
    "# Setup the pipeline with the preprocessing and the KNN model\n",
    "knn_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn\", KNeighborsRegressor(n_neighbors=5)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# GroupShuffleSplit\n",
    "group_split = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in group_split.split(\n",
    "    merged_gdf, groups=merged_gdf[\"orgnr_foretak\"]\n",
    "):\n",
    "    X_train, X_test = merged_gdf.iloc[train_idx], merged_gdf.iloc[test_idx]\n",
    "    y_train, y_test = (\n",
    "        merged_gdf[target].iloc[train_idx],\n",
    "        merged_gdf[target].iloc[test_idx],\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "knn_pipeline.fit(X_train[features], y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_pipeline.predict(X_test[features])\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e386fa6-0c54-4f31-a133-890318a0ab75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "adjusted_result_df = pd.DataFrame(\n",
    "    X_test, columns=features + [\"orgnr_bedrift\", \"reg_type\", \"orgnr_foretak\"]\n",
    ")\n",
    "adjusted_result_df[\"actual_omsetning\"] = y_test\n",
    "\n",
    "# Add the predicted 'omsetning' from y_pred\n",
    "adjusted_result_df[\"predicted_omsetning\"] = y_pred\n",
    "\n",
    "# adjusted_result_df[\"diff\"] = (adjusted_result_df[\"predicted_omsetning\"] - adjusted_result_df[\"actual_omsetning\"]).abs()\n",
    "# # adjusted_result_df.sort_values(by='diff', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e21c9ab-2b72-4127-891e-cdc26eabb819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# adjusted_result_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de167644-fb83-4ccd-a801-85d98dd0c98c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by 'orgnr_foretak' and sum 'predicted_omsetning'\n",
    "total_predicted = (\n",
    "    adjusted_result_df.groupby(\"orgnr_foretak\")[\"predicted_omsetning\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "total_predicted.rename(\n",
    "    columns={\"predicted_omsetning\": \"total_predicted_for_foretak\"}, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46ea9a3-5971-456f-a10e-07b824119ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adjusted_result_df = adjusted_result_df.merge(\n",
    "    total_predicted, on=\"orgnr_foretak\", how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb0a60-6f06-4eae-9f35-293fde629a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adjusted_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5393f2-ebc3-4c45-a199-8940f94b0152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now 'total_predicted' is a DataFrame with 'orgnr_foretak' and the corresponding 'total_predicted_for_foretak'\n",
    "\n",
    "# Calculate the percentage share of each 'orgnr_bedrift' prediction relative to its 'orgnr_foretak'\n",
    "adjusted_result_df[\"predicted_share\"] = (\n",
    "    adjusted_result_df[\"predicted_omsetning\"]\n",
    "    / adjusted_result_df[\"total_predicted_for_foretak\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0a941-d5e8-42e1-a336-973b3111d7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adjusted_result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919a0b6-6b1b-4250-b2f6-6560a3c4bbcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the new adjusted predicted 'omsetning' based on the percentage share\n",
    "adjusted_result_df[\"new_predicted\"] = (\n",
    "    adjusted_result_df[\"predicted_share\"] * adjusted_result_df[\"omsetning_foretak\"]\n",
    ")\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "# Now adjusted_result_df contains the adjusted predictions displayed as whole numbers\n",
    "# adjusted_result_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879dc67-2dee-4cc8-be64-4f27ada8bcc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only rows where reg_type is '02'\n",
    "reg_type_02_df = adjusted_result_df[adjusted_result_df[\"reg_type\"] == \"02\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4de80-0957-4b53-b0ee-a268ca0bec14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reg_type_02_df[\"diff\"] = (\n",
    "    reg_type_02_df[\"actual_omsetning\"] - reg_type_02_df[\"new_predicted\"]\n",
    ").abs()\n",
    "\n",
    "# Continue with your analysis using the filtered DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e1f44-0101-4aa2-b302-659723d8db02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by 'orgnr_foretak' and sum 'diff' for each group\n",
    "sum_diff_per_orgnr = reg_type_02_df.groupby(\"orgnr_foretak\")[\"diff\"].sum().reset_index()\n",
    "\n",
    "# Rename the summed 'diff' column for clarity\n",
    "sum_diff_per_orgnr = sum_diff_per_orgnr.rename(columns={\"diff\": \"sum_diff\"})\n",
    "\n",
    "# Merge this sum back into the original DataFrame\n",
    "reg_type_02_df_merged = reg_type_02_df.merge(sum_diff_per_orgnr, on=\"orgnr_foretak\")\n",
    "\n",
    "# Sort the merged DataFrame by 'sum_diff'\n",
    "sorted_reg_type_02_df = reg_type_02_df_merged.sort_values(by=\"sum_diff\")\n",
    "\n",
    "\n",
    "# Set the maximum number of rows to display\n",
    "pd.set_option(\"display.max_rows\", None)  # This will display all rows\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "# sorted_reg_type_02_df.head(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a222cee-c3f8-45b0-8a46-a3f08014aa01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) using the adjusted predictions\n",
    "mae_new_predicted = mean_absolute_error(\n",
    "    sorted_df[\"actual_omsetning\"], sorted_df[\"new_predicted\"]\n",
    ")\n",
    "r2_new_predicted = r2_score(sorted_df[\"actual_omsetning\"], sorted_df[\"new_predicted\"])\n",
    "print(\n",
    "    f\"Mean Absolute Error (MAE) for reg_type using new_predicted: {mae_new_predicted}\"\n",
    ")\n",
    "print(f\"R-squared (R2) for reg_type 2 using new_predicted: {r2_new_predicted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9df65ad-72af-4b29-8e1e-259b0a08a2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adjusted_result_df = adjusted_result_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005b8861-500c-45cb-9924-0667927d6570",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mae_new_predicted = mean_absolute_error(\n",
    "    adjusted_result_df[\"actual_omsetning\"], adjusted_result_df[\"new_predicted\"]\n",
    ")\n",
    "r2_new_predicted = r2_score(\n",
    "    adjusted_result_df[\"actual_omsetning\"], adjusted_result_df[\"new_predicted\"]\n",
    ")\n",
    "print(\n",
    "    f\"Mean Absolute Error (MAE) for the whole naring new_predicted: {mae_new_predicted}\"\n",
    ")\n",
    "print(f\"R-squared (R2) for the whole naring using new_predicted: {r2_new_predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbae69c-ef6f-43ed-bb69-f97fd6f5916f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get results for entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfe541-6038-447e-9299-c070a5f14417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Create a DataFrame with actual and predicted values\n",
    "# results_df = pd.DataFrame({\n",
    "#     'Actual': y_test,\n",
    "#     'Predicted': y_pred\n",
    "# })\n",
    "\n",
    "# # Reset the index of the DataFrame if necessary (to ensure alignment)\n",
    "# results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "# # Display the results\n",
    "# results_df.head(40)\n",
    "\n",
    "# Convert the predictions to a DataFrame\n",
    "predicted_df = pd.DataFrame(y_pred, columns=[\"Predicted\"])\n",
    "\n",
    "# Reset the index on the test set to align with the predicted DataFrame\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Convert the actual and predicted values to a DataFrame\n",
    "results_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "\n",
    "# Combine the test set with the actual and predicted values\n",
    "results_df = pd.concat([X_test, results_df], axis=1)\n",
    "\n",
    "# Calculate the difference between the actual and predicted values\n",
    "results_df[\"Difference\"] = results_df[\"Actual\"] - results_df[\"Predicted\"]\n",
    "\n",
    "# Display the results\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8dffe-4856-4049-985e-04bd43145924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where 'reg_type' is '02'\n",
    "filtered_df = results_df[results_df[\"reg_type\"] == \"02\"]\n",
    "\n",
    "# Extract the actual and predicted values for the filtered rows\n",
    "filtered_actual = filtered_df[\"Actual\"]\n",
    "filtered_predicted = filtered_df[\"Predicted\"]\n",
    "\n",
    "# Evaluate the model on the filtered data\n",
    "mse_filtered = mean_squared_error(filtered_actual, filtered_predicted)\n",
    "print(f\"Mean Squared Error for 'reg_type' 02: {mse_filtered}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) for the filtered data\n",
    "mae_filtered = mean_absolute_error(filtered_actual, filtered_predicted)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for the filtered data\n",
    "rmse_filtered = mean_squared_error(filtered_actual, filtered_predicted, squared=False)\n",
    "\n",
    "# Calculate R^2 Score for the filtered data\n",
    "r2_filtered = r2_score(filtered_actual, filtered_predicted)\n",
    "\n",
    "# Print the evaluation metrics for the filtered data\n",
    "print(f\"Mean Absolute Error for 'reg_type' 02: {mae_filtered}\")\n",
    "print(f\"Root Mean Squared Error for 'reg_type' 02: {rmse_filtered}\")\n",
    "print(f\"R^2 Score for 'reg_type' 02: {r2_filtered}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0090e28-fb2c-4728-9085-c37b3ac965a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get results just for chosen foretak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c769b3-0a9e-43d6-8a94-cbe55c7f1d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# Define your features as before\n",
    "features = [\n",
    "    \"x_koordinat\",\n",
    "    \"y_koordinat\",\n",
    "    \"sysselsetting_syss\",\n",
    "    \"naring\",\n",
    "    \"omsetning_foretak\",\n",
    "    \"nopost_driftskostnader_foretak\",\n",
    "    \"oms_per_syss_foretak\",\n",
    "    \"oms_per_bedrift_foretak\",\n",
    "]\n",
    "\n",
    "# Reset the index of the DataFrame to make sure it's in sequential order after dropping NaNs\n",
    "merged_gdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Fit the NearestNeighbors model using only the features\n",
    "nn = NearestNeighbors()\n",
    "nn.fit(merged_gdf[features])\n",
    "\n",
    "# Find the instances of the specific 'orgnr_foretak' in the dataset\n",
    "orgnr_foretak_value = (\n",
    "    f\"{enhet}\"  # Replace with the actual 'orgnr_foretak' you're interested in\n",
    ")\n",
    "specific_orgnr_indices = merged_gdf.index[\n",
    "    merged_gdf[\"orgnr_foretak\"] == orgnr_foretak_value\n",
    "].tolist()\n",
    "\n",
    "# For each instance of 'orgnr_foretak', find its nearest neighbors\n",
    "all_neighbors_indices = []\n",
    "for index in specific_orgnr_indices:\n",
    "    distances, indices = nn.kneighbors(\n",
    "        [merged_gdf.loc[index, features].values], n_neighbors=5\n",
    "    )\n",
    "    all_neighbors_indices.extend(indices.flatten())\n",
    "\n",
    "# Remove duplicates and the indices of the 'orgnr_foretak' itself\n",
    "all_neighbors_indices = list(set(all_neighbors_indices) - set(specific_orgnr_indices))\n",
    "\n",
    "# Create a DataFrame of the nearest neighbors\n",
    "neighbors_df = merged_gdf.iloc[all_neighbors_indices]\n",
    "\n",
    "# Combine the specific 'orgnr_foretak' rows with the neighbors\n",
    "final_df = pd.concat([merged_gdf.iloc[specific_orgnr_indices], neighbors_df])\n",
    "\n",
    "final_df = final_df.drop_duplicates(subset=\"orgnr_bedrift\")\n",
    "\n",
    "\n",
    "# The resulting 'final_df' will have all the same columns as 'merged_gdf' but only contain the rows for the chosen 'orgnr_foretak' and its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7869409-3fde-42bf-bbe9-3e6637abd738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sg.explore(nærmeste_naboer, \"omsetning\")\n",
    "# final_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb6e78-6d4e-4edb-bb1a-2b53aa260a08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "# Define your features as before\n",
    "features = [\n",
    "    \"x_koordinat\",\n",
    "    \"y_koordinat\",\n",
    "    \"oms_per_syss_foretak\",\n",
    "    \"oms_per_bedrift_foretak\",\n",
    "]\n",
    "\n",
    "# Reset the index of the DataFrame to make sure it's in sequential order after dropping NaNs\n",
    "merged_gdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Fit the NearestNeighbors model using only the features\n",
    "nn = NearestNeighbors()\n",
    "nn.fit(merged_gdf[features])\n",
    "\n",
    "# Find the instances of the specific 'orgnr_foretak' in the dataset\n",
    "orgnr_foretak_value = (\n",
    "    f\"{enhet}\"  # Replace with the actual 'orgnr_foretak' you're interested in\n",
    ")\n",
    "specific_orgnr_indices = merged_gdf.index[\n",
    "    merged_gdf[\"orgnr_foretak\"] == orgnr_foretak_value\n",
    "].tolist()\n",
    "\n",
    "# For each instance of 'orgnr_foretak', find its nearest neighbors\n",
    "all_neighbors_indices = []\n",
    "orgnr_bedrift_neighbors = (\n",
    "    {}\n",
    ")  # This dictionary will map each 'orgnr_bedrift' to its neighbors\n",
    "for index in specific_orgnr_indices:\n",
    "    distances, indices = nn.kneighbors(\n",
    "        [merged_gdf.loc[index, features].values], n_neighbors=5\n",
    "    )\n",
    "    all_neighbors_indices.extend(indices.flatten())\n",
    "    orgnr_bedrift_neighbors[\n",
    "        merged_gdf.loc[index, \"orgnr_bedrift\"]\n",
    "    ] = indices.flatten()  # Add the neighbors to the dictionary\n",
    "\n",
    "# Remove duplicates and the indices of the 'orgnr_foretak' itself\n",
    "all_neighbors_indices = list(set(all_neighbors_indices) - set(specific_orgnr_indices))\n",
    "\n",
    "# Create a DataFrame of the nearest neighbors\n",
    "neighbors_df = merged_gdf.iloc[all_neighbors_indices]\n",
    "\n",
    "# Add a new column to 'neighbors_df' that indicates which 'orgnr_bedrift' each neighbor belongs to\n",
    "neighbors_df[\"orgnr_bedrift_neighbor\"] = neighbors_df.index.map(\n",
    "    lambda x: [k for k, v in orgnr_bedrift_neighbors.items() if x in v]\n",
    ")\n",
    "\n",
    "# Combine the specific 'orgnr_foretak' rows with the neighbors\n",
    "nærmeste_naboer = pd.concat([merged_gdf.iloc[specific_orgnr_indices], neighbors_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41729751-96e6-4faf-9fec-f4c21f130b25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# units of a foretak cant be neighbors to each other:\n",
    "\n",
    "# Filter out the instances of the specific 'orgnr_foretak'\n",
    "orgnr_foretak_value = f\"{enhet}\"  # The 'orgnr_foretak' you're interested in\n",
    "filtered_gdf = merged_gdf[merged_gdf[\"orgnr_foretak\"] != orgnr_foretak_value]\n",
    "\n",
    "# Fit the NearestNeighbors model on the filtered DataFrame\n",
    "nn = NearestNeighbors()\n",
    "nn.fit(filtered_gdf[features])\n",
    "\n",
    "# Find the instances of the specific 'orgnr_foretak' in the original DataFrame\n",
    "specific_orgnr_indices = merged_gdf.index[\n",
    "    merged_gdf[\"orgnr_foretak\"] == orgnr_foretak_value\n",
    "].tolist()\n",
    "\n",
    "# For each instance of 'orgnr_foretak', find its nearest neighbors in the filtered DataFrame\n",
    "all_neighbors_indices = []\n",
    "for index in specific_orgnr_indices:\n",
    "    distances, indices = nn.kneighbors(\n",
    "        [merged_gdf.loc[index, features].values], n_neighbors=5\n",
    "    )\n",
    "    all_neighbors_indices.extend(indices.flatten())\n",
    "\n",
    "# Remove duplicates from the neighbors' indices\n",
    "all_neighbors_indices = list(set(all_neighbors_indices))\n",
    "\n",
    "# Create a DataFrame of the nearest neighbors\n",
    "neighbors_df = filtered_gdf.iloc[all_neighbors_indices]\n",
    "\n",
    "# Combine the specific 'orgnr_foretak' rows with the neighbors\n",
    "nærmeste_naboer = pd.concat([merged_gdf.iloc[specific_orgnr_indices], neighbors_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb19a96b-bfc9-4760-9e13-112764c0f8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# final_df.head(200)\n",
    "\n",
    "sg.explore(nærmeste_naboer, \"orgnr_foretak\")\n",
    "\n",
    "# nærmeste_naboer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890de4ba-992d-4f8f-b26b-ccc7cd5dec41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sg.explore(final_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00864e0-4d2a-4624-a87d-112afc67cf0d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Train the model to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf91b6-5778-4455-947c-076fcd4fcc53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score,\n",
    "    make_scorer,\n",
    ")\n",
    "\n",
    "# Assume 'merged_gdf' is your DataFrame and it has been preprocessed to include the following columns:\n",
    "# 'x_koordinat', 'y_koordinat', 'sysselsetting_syss', 'naring', 'omsetning'\n",
    "\n",
    "# Define your features and target variable\n",
    "features = [\n",
    "    \"x_koordinat\",\n",
    "    \"y_koordinat\",\n",
    "    \"sysselsetting_syss\",\n",
    "    \"naring\",\n",
    "    \"omsetning_foretak\",\n",
    "    \"nopost_driftskostnader_foretak\",\n",
    "    \"oms_per_syss_foretak\",\n",
    "    \"oms_per_bedrift_foretak\",\n",
    "]\n",
    "target = \"omsetning\"\n",
    "\n",
    "# Preprocessing\n",
    "# Define the preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            [\n",
    "                \"x_koordinat\",\n",
    "                \"y_koordinat\",\n",
    "                \"sysselsetting_syss\",\n",
    "                \"naring\",\n",
    "                \"omsetning_foretak\",\n",
    "                \"nopost_driftskostnader_foretak\",\n",
    "                \"oms_per_syss_foretak\",\n",
    "                \"oms_per_bedrift_foretak\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            [\"naring\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model setup\n",
    "# Setup the pipeline with the preprocessing and the KNN model\n",
    "knn_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn\", KNeighborsRegressor()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    merged_gdf[features + [\"orgnr_bedrift\", \"reg_type\", \"orgnr_foretak\"]],\n",
    "    merged_gdf[target],\n",
    "    test_size=0.2,  # 20% for testing\n",
    "    random_state=42,  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "# param_grid = {\n",
    "#     'knn__n_neighbors': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "#     'knn__weights': ['uniform', 'distance'],\n",
    "#     # Add other parameters here if you wish\n",
    "# }\n",
    "\n",
    "param_grid = {\n",
    "    \"knn__n_neighbors\": [2, 3, 4, 5],\n",
    "    \"knn__weights\": [\"uniform\", \"distance\"],\n",
    "    # 'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "    # 'knn__leaf_size': [10, 20, 30, 40, 50],\n",
    "    \"knn__p\": [1, 2],\n",
    "    \"knn__metric\": [\"euclidean\", \"manhattan\", \"minkowski\"],\n",
    "}\n",
    "\n",
    "\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    return adjusted_r2\n",
    "\n",
    "\n",
    "# Create a scorer\n",
    "adjusted_r2_scorer = make_scorer(\n",
    "    adjusted_r2_score, greater_is_better=True, n=X_train.shape[0], p=X_train.shape[1]\n",
    ")\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(knn_pipeline, param_grid, cv=5, scoring=adjusted_r2_scorer)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "# Use the best estimator to make predictions on the test set\n",
    "y_pred = grid_search.predict(X_test)\n",
    "\n",
    "\n",
    "predicted_df = pd.DataFrame(y_pred, columns=[\"Predicted\"])\n",
    "\n",
    "# Reset the index on the test set to align with the predicted DataFrame\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Convert the actual and predicted values to a DataFrame\n",
    "results_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "\n",
    "# Combine the test set with the actual and predicted values\n",
    "results_df = pd.concat([X_test, results_df], axis=1)\n",
    "\n",
    "# Calculate the difference between the actual and predicted values\n",
    "results_df[\"Difference\"] = results_df[\"Actual\"] - results_df[\"Predicted\"]\n",
    "\n",
    "# Filter the DataFrame for rows where 'reg_type' is '02'\n",
    "filtered_df = results_df[results_df[\"reg_type\"] == \"02\"]\n",
    "\n",
    "# Extract the actual and predicted values for the filtered rows\n",
    "filtered_actual = filtered_df[\"Actual\"]\n",
    "filtered_predicted = filtered_df[\"Predicted\"]\n",
    "\n",
    "# Evaluate the model on the filtered data\n",
    "mse_filtered = mean_squared_error(filtered_actual, filtered_predicted)\n",
    "print(f\"Mean Squared Error for 'reg_type' 02: {mse_filtered}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) for the filtered data\n",
    "mae_filtered = mean_absolute_error(filtered_actual, filtered_predicted)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for the filtered data\n",
    "rmse_filtered = mean_squared_error(filtered_actual, filtered_predicted, squared=False)\n",
    "\n",
    "# Calculate R^2 Score for the filtered data\n",
    "r2_filtered = r2_score(filtered_actual, filtered_predicted)\n",
    "\n",
    "# Number of observations\n",
    "n = X_test.shape[0]\n",
    "\n",
    "# Number of features (predictors)\n",
    "p = X_test.shape[1]\n",
    "\n",
    "# Calculate Adjusted R^2 Score\n",
    "adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "\n",
    "# Print the evaluation metrics for the filtered data\n",
    "print(f\"Mean Absolute Error for 'reg_type' 02: {mae_filtered}\")\n",
    "print(f\"Root Mean Squared Error for 'reg_type' 02: {rmse_filtered}\")\n",
    "print(f\"R^2 Score for 'reg_type' 02: {r2_filtered}\")\n",
    "print(f\"Adjusted R^2 Score: {adjusted_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d72ca51-5d7a-48fd-85f0-a1b5637ac2d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "With Everything:\n",
    "    \n",
    "Best parameters: {'knn__n_neighbors': 4, 'knn__weights': 'uniform'}\n",
    "Mean Squared Error: 6194897820.853139\n",
    "Mean Absolute Error: 13858.122807017544\n",
    "Root Mean Squared Error: 78707.67320187492\n",
    "R^2 Score: 0.8251536639477731\n",
    "Adjusted R^2 Score: 0.824382991392171\n",
    "\n",
    "Without driftskostnader:\n",
    "\n",
    "Best parameters: {'knn__n_neighbors': 5, 'knn__weights': 'uniform'}\n",
    "Mean Squared Error: 8963619983.572657\n",
    "Mean Absolute Error: 15219.74111842105\n",
    "Root Mean Squared Error: 94676.39612687344\n",
    "R^2 Score: 0.747008561365365\n",
    "\n",
    "Without geopgraphical data:\n",
    "\n",
    "Best parameters: {'knn__n_neighbors': 5, 'knn__weights': 'uniform'}\n",
    "Mean Squared Error: 8692302957.268972\n",
    "Mean Absolute Error: 12964.328618421052\n",
    "Root Mean Squared Error: 93232.52092091565\n",
    "R^2 Score: 0.7546662805610065\n",
    "\n",
    "Without per foretak data and without driftskostnader:\n",
    "\n",
    "Best parameters: {'knn__n_neighbors': 10, 'knn__weights': 'uniform'}\n",
    "Mean Squared Error: 22358440708.48245\n",
    "Mean Absolute Error: 22249.31398026316\n",
    "Root Mean Squared Error: 149527.3911645704\n",
    "R^2 Score: 0.3689498114787729\n",
    "\n",
    "Without syss and without driftskostnader:\n",
    "\n",
    "Best parameters: {'knn__n_neighbors': 7, 'knn__weights': 'uniform'}\n",
    "Mean Squared Error: 3511909656.662437\n",
    "Mean Absolute Error: 16206.316885964912\n",
    "Root Mean Squared Error: 59261.367320223355\n",
    "R^2 Score: 0.9008789888435484\n",
    "\n",
    "\n",
    "Without syss:\n",
    "\n",
    "Best parameters: {'knn__n_neighbors': 4, 'knn__weights': 'uniform'}\n",
    "Mean Squared Error: 3208207771.3834634\n",
    "Mean Absolute Error: 15201.000959429824\n",
    "Root Mean Squared Error: 56641.04316997934\n",
    "R^2 Score: 0.9094507463492871\n",
    "\n",
    "Without syss or geographical location:\n",
    "\n",
    "Best parameters: {'knn__n_neighbors': 3, 'knn__weights': 'uniform'}\n",
    "Mean Squared Error: 3714965103.26931\n",
    "Mean Absolute Error: 12404.040387426901\n",
    "Root Mean Squared Error: 60950.51356034098\n",
    "R^2 Score: 0.8951479014420503\n",
    "Adjusted R^2 Score: 0.8948595293338051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4843a-15dd-4872-bb36-a2ba78280680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1801e972-0380-421a-bee9-23f838572090",
   "metadata": {},
   "source": [
    "# Train data only for reg_type 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd581074-c9ac-4a48-943f-6b9bbc2b0f56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = merged_gdf[merged_gdf[\"reg_type\"] == \"02\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d7967a-4ff0-4733-99ac-9bc4588e592f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "\n",
    "# Replace 'target_variable_name' with the actual name of your target variable\n",
    "features = [\n",
    "    \"x_koordinat\",\n",
    "    \"y_koordinat\",\n",
    "    # \"sysselsetting_syss\",\n",
    "    \"naring\",\n",
    "    # \"omsetning_foretak\",\n",
    "    # \"nopost_driftskostnader_foretak\",\n",
    "    \"oms_per_syss_foretak\",\n",
    "    # \"oms_per_bedrift_foretak\",\n",
    "]\n",
    "target = \"omsetning\"\n",
    "\n",
    "# Preprocessing\n",
    "# Define the preprocessing for numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            [\n",
    "                \"x_koordinat\",\n",
    "                \"y_koordinat\",\n",
    "                # \"sysselsetting_syss\",\n",
    "                \"naring\",\n",
    "                # \"omsetning_foretak\",\n",
    "                # \"nopost_driftskostnader_foretak\",\n",
    "                \"oms_per_syss_foretak\",\n",
    "                # \"oms_per_bedrift_foretak\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            [\"naring\"],\n",
    "        ),  # Set handle_unknown to 'ignore'\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Model setup\n",
    "# Setup the pipeline with the preprocessing and the KNN model\n",
    "knn_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knn\", KNeighborsRegressor(n_neighbors=4)),  # Adjust n_neighbors as needed\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    filtered_df[features + [\"orgnr_bedrift\", \"reg_type\", \"orgnr_foretak\"]],\n",
    "    filtered_df[target],\n",
    "    test_size=0.2,  # 20% for testing\n",
    "    random_state=42,  # Seed for reproducibility\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "knn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = knn_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Calculate R^2 Score\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, mae, rmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771316e6-b527-4215-9a7b-393a22471734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a DataFrame with actual and predicted values\n",
    "# results_df = pd.DataFrame({\n",
    "#     'Actual': y_test,\n",
    "#     'Predicted': y_pred\n",
    "# })\n",
    "\n",
    "# # Reset the index of the DataFrame if necessary (to ensure alignment)\n",
    "# results_df = results_df.reset_index(drop=True)\n",
    "\n",
    "# # Display the results\n",
    "# results_df.head(40)\n",
    "\n",
    "# Convert the predictions to a DataFrame\n",
    "predicted_df = pd.DataFrame(y_pred, columns=[\"Predicted\"])\n",
    "\n",
    "# Reset the index on the test set to align with the predicted DataFrame\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "# Convert the actual and predicted values to a DataFrame\n",
    "results_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "\n",
    "# Combine the test set with the actual and predicted values\n",
    "results_df = pd.concat([X_test, results_df], axis=1)\n",
    "\n",
    "# Calculate the difference between the actual and predicted values\n",
    "results_df[\"Difference\"] = results_df[\"Actual\"] - results_df[\"Predicted\"]\n",
    "\n",
    "# Display the results\n",
    "\n",
    "\n",
    "# Filter the DataFrame for rows where 'reg_type' is '02'\n",
    "filtered_df = results_df[results_df[\"reg_type\"] == \"02\"]\n",
    "\n",
    "# Extract the actual and predicted values for the filtered rows\n",
    "filtered_actual = filtered_df[\"Actual\"]\n",
    "filtered_predicted = filtered_df[\"Predicted\"]\n",
    "\n",
    "# Evaluate the model on the filtered data\n",
    "mse_filtered = mean_squared_error(filtered_actual, filtered_predicted)\n",
    "print(f\"Mean Squared Error for 'reg_type' 02: {mse_filtered}\")\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE) for the filtered data\n",
    "mae_filtered = mean_absolute_error(filtered_actual, filtered_predicted)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE) for the filtered data\n",
    "rmse_filtered = mean_squared_error(filtered_actual, filtered_predicted, squared=False)\n",
    "\n",
    "# Calculate R^2 Score for the filtered data\n",
    "r2_filtered = r2_score(filtered_actual, filtered_predicted)\n",
    "\n",
    "# Print the evaluation metrics for the filtered data\n",
    "print(f\"Mean Absolute Error for 'reg_type' 02: {mae_filtered}\")\n",
    "print(f\"Root Mean Squared Error for 'reg_type' 02: {rmse_filtered}\")\n",
    "print(f\"R^2 Score for 'reg_type' 02: {r2_filtered}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee0d4f-bde2-412f-b5fa-9e38e2310109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "# Define a function to calculate adjusted R^2\n",
    "def adjusted_r2_score(y_true, y_pred, n, p):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    return adjusted_r2\n",
    "\n",
    "\n",
    "# Create a scorer\n",
    "adjusted_r2_scorer = make_scorer(\n",
    "    adjusted_r2_score, greater_is_better=True, n=X_train.shape[0], p=X_train.shape[1]\n",
    ")\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(knn_pipeline, param_grid, cv=5, scoring=adjusted_r2_scorer)\n",
    "\n",
    "# Fit the GridSearchCV object to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found by GridSearchCV\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8510d9-8a29-47ec-94f2-f3e21d109da3",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8df50-8b49-4ecb-84b1-f8a89af6762f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Assuming 'merged_gdf' is your DataFrame and it has the same columns as before\n",
    "\n",
    "# Features and target variable\n",
    "features = [\n",
    "    \"x_koordinat\",\n",
    "    \"y_koordinat\",\n",
    "    \"sysselsetting_syss\",\n",
    "    \"naring\",\n",
    "    \"omsetning_foretak\",\n",
    "    \"nopost_driftskostnader_foretak\",\n",
    "    \"oms_per_syss_foretak\",\n",
    "    \"oms_per_bedrift_foretak\",\n",
    "]\n",
    "target = \"omsetning\"\n",
    "\n",
    "# Preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            [\n",
    "                \"x_koordinat\",\n",
    "                \"y_koordinat\",\n",
    "                \"sysselsetting_syss\",\n",
    "                \"omsetning_foretak\",\n",
    "                \"nopost_driftskostnader_foretak\",\n",
    "                \"oms_per_syss_foretak\",\n",
    "                \"oms_per_bedrift_foretak\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "            [\"naring\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Model setup\n",
    "linear_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"linear_reg\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# GroupShuffleSplit\n",
    "group_split = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in group_split.split(\n",
    "    merged_gdf, groups=merged_gdf[\"orgnr_foretak\"]\n",
    "):\n",
    "    X_train, X_test = merged_gdf.iloc[train_idx], merged_gdf.iloc[test_idx]\n",
    "    y_train, y_test = (\n",
    "        merged_gdf[target].iloc[train_idx],\n",
    "        merged_gdf[target].iloc[test_idx],\n",
    "    )\n",
    "\n",
    "# Train the model\n",
    "linear_pipeline.fit(X_train[features], y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = linear_pipeline.predict(X_test[features])\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c68305-809b-471a-a73d-efc5f17a1fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Assume knn_pipeline and linear_pipeline are your trained KNN and linear models\n",
    "\n",
    "# Create a stacking regressor\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[(\"knn\", knn_pipeline), (\"linear\", linear_pipeline)],\n",
    "    final_estimator=Ridge(),\n",
    ")\n",
    "\n",
    "# Train the stacked model\n",
    "stacked_model.fit(X_train[features], y_train)\n",
    "\n",
    "\n",
    "# Make predictions on the test set using the stacked model\n",
    "y_pred_stacked = stacked_model.predict(X_test[features])\n",
    "\n",
    "# Evaluate the stacked model\n",
    "stacked_mse = mean_squared_error(y_test, y_pred_stacked)\n",
    "stacked_r2 = r2_score(y_test, y_pred_stacked)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Stacked Model - Mean Squared Error: {stacked_mse}\")\n",
    "print(f\"Stacked Model - R^2 Score: {stacked_r2}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebba8c69-0cff-4cd7-a56b-bb9c056aac8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
