{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overall strategy:\n",
    "\n",
    "#### 1) split into good and bad data. Currently only splitting based on > 50% fordelt. Need to split those who only deliver to 1 bedrift into bad as well. \n",
    "#### 2) for the bad data, use ml to guess the oms , distribute drkost based on profit level and oms. \n",
    "#### 3) merge this data back with the good data.\n",
    "#### 4) run the regular AO with 99 on th entire df\n",
    "#### 5) create an update file. Send to google cloud\n",
    "#### 6) send to linux. Use the sas code to update isee\n",
    "\n",
    "### How to organise functions in final notebook\n",
    "\n",
    "#### Choose naring\n",
    "#### Choose parameters for separating between good, bad dfs\n",
    "#### Apply which machine learning model? Varehandels\n",
    "#### Bring back everything to a final DF. Apply 999 logic to this df. \n",
    "#### Further updates to rad 0 level data (foretak) \n",
    "#### Need to think about what to do with reg_types for example reg_type 03\n",
    "\n",
    "- (A) not 100% fordelt. automatically adjust. Create a list of things of foretak to check naring. \n",
    "- avanse\n",
    "- 1000 feils\n",
    "- IKT investering\n",
    "\n",
    "\n",
    "- Need to bring in past years data and data from other sources. \n",
    "\n",
    "#### Create updatefile. \n",
    "\n",
    "## FEATURE ENGINEERING!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gcsfs\n",
    "import getpass\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import geopandas as gpd\n",
    "\n",
    "# import sgis as sg\n",
    "import dapla as dp\n",
    "import datetime\n",
    "from dapla.auth import AuthClient\n",
    "from dapla import FileClient\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "import requests\n",
    "from pyjstat import pyjstat\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../functions\")\n",
    "import kommune_pop\n",
    "import kommune_inntekt\n",
    "import kpi\n",
    "import ao\n",
    "\n",
    "fs = FileClient.get_gcs_file_system()\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# good_df = ao.rette_bedrifter(good_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ã…rgang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "year = 2021\n",
    "fjor = year - 1\n",
    "limit = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# get dynarev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fil_path = [\n",
    "    f\n",
    "    for f in fs.glob(\n",
    "        f\"gs://ssb-prod-noeku-data-produkt/eimerdb/nokubasen/skjemadata/aar={year}/skjema=RA-0174-1/*\"\n",
    "    )\n",
    "    if f.endswith(\".parquet\")\n",
    "]\n",
    "\n",
    "# Use the ParquetDataset to read multiple files\n",
    "dataset = pq.ParquetDataset(fil_path, filesystem=fs)\n",
    "table = dataset.read()\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "skjema = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "felt_id_values = [\n",
    "    \"V_ORGNR\",\n",
    "    \"F_ADRESSE\",\n",
    "    \"FJOR_NACE_B_T1\",\n",
    "    \"TMP_SN2007_5\",\n",
    "    \"B_KOMMUNENR\",\n",
    "    \"REGTYPE\",\n",
    "    \"B_SYSSELSETTING_SYSS\",\n",
    "    \"TMP_NY_BDR_SYSS\",\n",
    "    \"GJELDENDE_BDR_SYSS\",\n",
    "    \"FJOR_SYSSEL_T1\",\n",
    "    \"LONN_PST_AORDN\",\n",
    "    \"GJELDENDE_LONN_KR\",\n",
    "    \"LONN\",\n",
    "    \"FJOR_LONN_KR_T1\",\n",
    "    \"TMP_SNITTLONN\",\n",
    "    \"FJOR_SNITTLONN_T1\",\n",
    "    \"GJELDENDE_OMSETN_KR\",\n",
    "    \"OMSETN_KR\",\n",
    "    \"FJOR_OMSETN_KR_T1\",\n",
    "    \"TMP_SNITTOMS\",\n",
    "    \"FJOR_SNITTOMS_T1\",\n",
    "    \"TMP_SALGSINT_BED\",\n",
    "    \"TMP_FORBRUK_BED\",\n",
    "    \"VAREKOST_BED\",\n",
    "    \"GJELDENDE_DRIFTSK_KR\",\n",
    "    \"DRIFTSKOST_KR\",\n",
    "    \"FJOR_DRIFTSKOST_KR_T1\",\n",
    "    \"NACEF_5\",\n",
    "    \"SALGSINT\",\n",
    "    \"FORBRUK\",\n",
    "    \"TMP_NO_P4005\",\n",
    "    \"TMP_AVPROS_ORGFORB\",\n",
    "    \"ORGNR_N_1\",\n",
    "    \"TMP_NO_OMSETN\",\n",
    "    \"TMP_DRIFTSKOSTNAD_9010\",\n",
    "    \"TMP_DRIFTSKOSTNAD_9910\",\n",
    "]\n",
    "\n",
    "# Assuming `skjema` is your DataFrame and `felt_id_values` is your list of values\n",
    "skjema = skjema[skjema[\"feltnavn\"].isin(felt_id_values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pivot_df = skjema.pivot_table(\n",
    "    index=[\"id\", \"radnr\", \"lopenr\"],\n",
    "    columns=\"feltnavn\",\n",
    "    values=\"feltverdi\",\n",
    "    aggfunc=\"first\",\n",
    ")\n",
    "pivot_df = pivot_df.reset_index()\n",
    "pivot_df.columns = pivot_df.columns.str.lower()\n",
    "\n",
    "pivot_df[\"year\"] = year\n",
    "\n",
    "pivot_df\n",
    "# test = pivot_df['FJOR_OMSETN_KR_T1']\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming your DataFrame is named pivot_df\n",
    "\n",
    "# Create the 'foretak' DataFrame\n",
    "foretak = pivot_df.loc[pivot_df[\"radnr\"] == 0]\n",
    "\n",
    "# Create the 'bedrift' DataFrame\n",
    "bedrift = pivot_df.loc[pivot_df[\"radnr\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)\n",
    "# Assuming 'foretak' is your existing DataFrame\n",
    "\n",
    "selected_columns = [\n",
    "    \"id\",\n",
    "    \"lopenr\",\n",
    "    \"forbruk\",\n",
    "    \"nacef_5\",\n",
    "    \"orgnr_n_1\",\n",
    "    \"salgsint\",\n",
    "    \"tmp_driftskostnad_9010\",\n",
    "    \"tmp_driftskostnad_9910\",\n",
    "    \"tmp_no_omsetn\",\n",
    "    \"tmp_no_p4005\",\n",
    "]\n",
    "\n",
    "foretak = foretak[selected_columns]\n",
    "\n",
    "# Assuming 'foretak' is your DataFrame\n",
    "foretak.rename(columns={\"tmp_no_omsetn\": \"foretak_omsetning\"}, inplace=True)\n",
    "\n",
    "\n",
    "foretak = foretak.fillna(0)\n",
    "\n",
    "foretak[[\"tmp_driftskostnad_9010\", \"tmp_driftskostnad_9910\"]] = foretak[\n",
    "    [\"tmp_driftskostnad_9010\", \"tmp_driftskostnad_9910\"]\n",
    "].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "foretak[\"foretak_driftskostnad\"] = foretak[\n",
    "    [\"tmp_driftskostnad_9010\", \"tmp_driftskostnad_9910\"]\n",
    "].max(axis=1)\n",
    "\n",
    "# Drop the specified columns\n",
    "foretak.drop([\"tmp_driftskostnad_9010\", \"tmp_driftskostnad_9910\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'bedrift' is your DataFrame\n",
    "columns_to_drop = [\n",
    "    \"forbruk\",\n",
    "    \"nacef_5\",\n",
    "    \"orgnr_n_1\",\n",
    "    \"salgsint\",\n",
    "    \"tmp_driftskostnad_9010\",\n",
    "    \"tmp_driftskostnad_9910\",\n",
    "    \"tmp_no_omsetn\",\n",
    "    \"tmp_no_p4005\",\n",
    "]\n",
    "\n",
    "bedrift.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'bedrift' is your DataFrame\n",
    "columns_to_fill = [\"omsetn_kr\", \"driftskost_kr\"]\n",
    "\n",
    "# Convert columns to numeric, replacing non-convertible values with NaN\n",
    "bedrift[columns_to_fill] = bedrift[columns_to_fill].apply(\n",
    "    pd.to_numeric, errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Fill NaN values with 0 for the specified columns\n",
    "bedrift[columns_to_fill] = bedrift[columns_to_fill].fillna(0)\n",
    "\n",
    "\n",
    "# hjelpe virksomheter\n",
    "if_condition = bedrift[\"regtype\"] == \"04\"\n",
    "\n",
    "# If the condition is True, set 'omsetn_kr' equal to 'driftskost_kr'\n",
    "bedrift.loc[if_condition, \"omsetn_kr\"] = bedrift.loc[if_condition, \"driftskost_kr\"]\n",
    "\n",
    "\n",
    "# Group by 'id' and calculate the sum\n",
    "grouped_bedrift = (\n",
    "    bedrift.groupby(\"id\")[[\"omsetn_kr\", \"driftskost_kr\"]].sum().reset_index()\n",
    ")\n",
    "\n",
    "# Rename the columns\n",
    "grouped_bedrift.rename(\n",
    "    columns={\"omsetn_kr\": \"tot_oms_fordelt\", \"driftskost_kr\": \"tot_driftskost_fordelt\"},\n",
    "    inplace=True,\n",
    ")\n",
    "\n",
    "# Merge the grouped DataFrame back to the original DataFrame based on 'id'\n",
    "bedrift = pd.merge(bedrift, grouped_bedrift, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'foretak' and 'bedrift' are your DataFrames\n",
    "merged_df = pd.merge(foretak, bedrift, on=[\"id\", \"lopenr\"], how=\"inner\")\n",
    "# merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'merged_df' is your DataFrame\n",
    "\n",
    "# Convert columns to numeric, replacing non-convertible values with NaN\n",
    "merged_df[\"tot_oms_fordelt\"] = pd.to_numeric(\n",
    "    merged_df[\"tot_oms_fordelt\"], errors=\"coerce\"\n",
    ")\n",
    "merged_df[\"foretak_omsetning\"] = pd.to_numeric(\n",
    "    merged_df[\"foretak_omsetning\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Calculate omsetning_percentage\n",
    "merged_df[\"omsetning_percentage\"] = (\n",
    "    merged_df[\"tot_oms_fordelt\"] / merged_df[\"foretak_omsetning\"]\n",
    ")\n",
    "\n",
    "# Convert columns to numeric, replacing non-convertible values with NaN\n",
    "merged_df[\"tot_driftskost_fordelt\"] = pd.to_numeric(\n",
    "    merged_df[\"tot_driftskost_fordelt\"], errors=\"coerce\"\n",
    ")\n",
    "merged_df[\"foretak_driftskostnad\"] = pd.to_numeric(\n",
    "    merged_df[\"foretak_driftskostnad\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Calculate driftskostnader_percentage\n",
    "merged_df[\"driftskostnader_percentage\"] = (\n",
    "    merged_df[\"tot_driftskost_fordelt\"] / merged_df[\"foretak_driftskostnad\"]\n",
    ")\n",
    "\n",
    "merged_df[\"driftskostnader_percentage\"] = (\n",
    "    merged_df[\"tot_driftskost_fordelt\"] / merged_df[\"foretak_driftskostnad\"]\n",
    ").round(4)\n",
    "\n",
    "# Fill NaN with a specific value (e.g., 0)\n",
    "merged_df[\"driftskostnader_percentage\"].fillna(0, inplace=True)\n",
    "merged_df[\"omsetning_percentage\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'merged_df' is your DataFrame\n",
    "\n",
    "# Create the 'Good' DataFrame\n",
    "good_temp_df = merged_df[\n",
    "    (merged_df[\"omsetning_percentage\"] >= limit)\n",
    "    & (merged_df[\"driftskostnader_percentage\"] >= limit)\n",
    "]\n",
    "\n",
    "# Create 'bedrift_count' and 'distribution_count'\n",
    "good_temp_df[\"bedrift_count\"] = good_temp_df.groupby(\"orgnr_n_1\")[\n",
    "    \"orgnr_n_1\"\n",
    "].transform(\"count\")\n",
    "good_temp_df[\"distribution_count\"] = good_temp_df.groupby(\"orgnr_n_1\")[\n",
    "    \"omsetn_kr\"\n",
    "].transform(lambda x: (x > 0).sum())\n",
    "\n",
    "# Create 'bad_temp' DataFrame based on conditions\n",
    "bad_temp = good_temp_df[\n",
    "    (good_temp_df[\"bedrift_count\"] > 5) & (good_temp_df[\"distribution_count\"] <= 2)\n",
    "]\n",
    "\n",
    "# Create 'good_df' by excluding rows from 'bad_temp'\n",
    "good_df = (\n",
    "    pd.merge(good_temp_df, bad_temp, how=\"outer\", indicator=True)\n",
    "    .query('_merge == \"left_only\"')\n",
    "    .drop(\"_merge\", axis=1)\n",
    ")\n",
    "\n",
    "\n",
    "good_df[\"oms_share\"] = good_df[\"omsetn_kr\"] / good_df[\"tot_oms_fordelt\"].round(5)\n",
    "\n",
    "# Round the values to whole numbers before assigning to the new columns\n",
    "good_df[\"new_oms\"] = (\n",
    "    (good_df[\"oms_share\"] * good_df[\"foretak_omsetning\"]).round(0).astype(int)\n",
    ")\n",
    "\n",
    "good_df[\"oms_share\"] = good_df[\"new_oms\"] / good_df[\"tot_oms_fordelt\"].round(5)\n",
    "\n",
    "\n",
    "# Create the 'Mixed' DataFrame\n",
    "onlygoodoms = merged_df[\n",
    "    (\n",
    "        (merged_df[\"omsetning_percentage\"] > limit)\n",
    "        & (merged_df[\"driftskostnader_percentage\"] <= limit)\n",
    "    )\n",
    "]\n",
    "\n",
    "onlygooddriftskostnader = merged_df[\n",
    "    (\n",
    "        (merged_df[\"driftskostnader_percentage\"] > limit)\n",
    "        & (merged_df[\"omsetning_percentage\"] <= limit)\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create the 'Bad' DataFrame\n",
    "bad_df = merged_df[\n",
    "    (merged_df[\"omsetning_percentage\"] <= limit)\n",
    "    & (merged_df[\"driftskostnader_percentage\"] <= limit)\n",
    "]\n",
    "bad_df = pd.concat([bad_df, bad_temp]).drop_duplicates(keep=False)\n",
    "bad_df = pd.concat([bad_df, onlygooddriftskostnader]).drop_duplicates(keep=False)\n",
    "\n",
    "\n",
    "del onlygooddriftskostnader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onlygoodoms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bad_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "good_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "good distribution delivered (both oms and dkost): 83.33%\n",
    "onlygoodoms: 11.96%\n",
    "onlygooddriftskostnader: 0.693%\n",
    "bad (bad distribution of both oms and dkost) : 4.02% "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Step 1. \n",
    "\n",
    "Fill all bad df with NAN. For onlygoodoms use profit margin to impute driftskostnader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fill bad_df with NaN\n",
    "bad_df[\"omsetn_kr\"] = np.nan\n",
    "bad_df[\"driftskost_kr\"] = np.nan\n",
    "\n",
    "bad_df[\"new_oms\"] = bad_df[\"omsetn_kr\"]\n",
    "\n",
    "good_df[\"new_oms\"] = good_df[\"omsetn_kr\"]\n",
    "\n",
    "# Correct driftskostnader for onlygoodoms\n",
    "\n",
    "onlygoodoms[\"oms_share\"] = onlygoodoms[\"omsetn_kr\"] / onlygoodoms[\n",
    "    \"tot_oms_fordelt\"\n",
    "].round(5)\n",
    "\n",
    "# Round the values to whole numbers before assigning to the new columns\n",
    "onlygoodoms[\"new_oms\"] = (\n",
    "    (onlygoodoms[\"oms_share\"] * onlygoodoms[\"foretak_omsetning\"]).round(0).astype(int)\n",
    ")\n",
    "\n",
    "onlygoodoms[\"margin\"] = (\n",
    "    onlygoodoms[\"foretak_driftskostnad\"] / onlygoodoms[\"foretak_omsetning\"]\n",
    ")\n",
    "\n",
    "onlygoodoms[\"old_drkost\"] = onlygoodoms[\"driftskost_kr\"]\n",
    "\n",
    "onlygoodoms[\"new_drkost\"] = onlygoodoms[\"margin\"] * onlygoodoms[\"new_oms\"]\n",
    "\n",
    "onlygoodoms[\"driftskost_kr\"] = onlygoodoms[\"new_drkost\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "# Step 2: \n",
    "\n",
    "merge bad dfs back into good_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'onlygoodoms','bad_df', 'good_df', and 'onlygooddriftskostnader' are your DataFrames\n",
    "# Merge all DataFrames into 'imputer' in a single step\n",
    "merged_df = pd.concat([onlygoodoms, bad_df, good_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'imputer' is your DataFrame and you want to select specific columns\n",
    "selected_columns = [\n",
    "    \"id\",\n",
    "    \"lopenr\",\n",
    "    \"forbruk\",\n",
    "    \"nacef_5\",\n",
    "    \"orgnr_n_1\",\n",
    "    \"salgsint\",\n",
    "    \"foretak_omsetning\",\n",
    "    \"tmp_no_p4005\",\n",
    "    \"foretak_driftskostnad\",\n",
    "    \"radnr\",\n",
    "    \"gjeldende_bdr_syss\",\n",
    "    \"fjor_driftskost_kr_t1\",\n",
    "    \"fjor_lonn_kr_t1\",\n",
    "    \"fjor_syssel_t1\",\n",
    "    \"gjeldende_lonn_kr\",\n",
    "    \"new_oms\",\n",
    "    \"b_kommunenr\",\n",
    "]\n",
    "\n",
    "# Create a new DataFrame with only the selected columns\n",
    "imputer = merged_df[selected_columns].copy()\n",
    "\n",
    "merged_df[\"n4\"] = merged_df[\"nacef_5\"].str[:5]\n",
    "\n",
    "# merged_df.head(100)\n",
    "\n",
    "# Assuming 'df' is your DataFrame\n",
    "# nan_count = imputer['new_oms'].isna().sum()\n",
    "# print(\"Number of NaN values in 'new_oms':\", nan_count)\n",
    "\n",
    "merged_df[\"new_oms\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "# step 3 . \n",
    "\n",
    "Feature engineering:\n",
    "\n",
    "- delta foretak oms\n",
    "- delta kommune population\n",
    "- delta kommune income\n",
    "- change in employees\n",
    "- consumer confidence index\n",
    "- inflation\n",
    "- gdp growth\n",
    "- past data\n",
    "- past years sales * foretak increase\n",
    "- past years sales * kommune pop\n",
    "- past years sales * kommune income"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Kommune belfolkning delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kommune_pop_aar = kommune_pop.fetch_population_data(year)\n",
    "# kommune_pop_fjor = kommune_pop.fetch_population_data(fjor)\n",
    "kommune_befolk = kommune_pop.befolkning_behandling(year, fjor)\n",
    "\n",
    "kommune_befolk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "tags": []
   },
   "source": [
    "## kommune inntekt delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kommune_innt_aar = kommune_inntekt.fetch_inntekt_data(year)\n",
    "# kommune_innt_fjor = kommune_inntekt.fetch_inntekt_data(fjor)\n",
    "kommune_inn = kommune_inntekt.inntekt_behandling(year, fjor)\n",
    "kommune_inn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## Inflasjon data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# kpi_total_data = kpi.fetch_kpi_data(year)\n",
    "# kpi_hovedgruppe_data = kpi.fetch_hovedgruppe_data(year)\n",
    "# kpi_gruppe_data = kpi.fetch_gruppe_data(year)\n",
    "# subgruppe1_data = kpi.fetch_subgruppe1_data(year)\n",
    "\n",
    "kpi_df = kpi.process_kpi_data(year)\n",
    "\n",
    "# kpi_df = pd.concat([kpi_total_data, kpi_hovedgruppe_data, kpi_gruppe_data, subgruppe1_data], ignore_index=True)\n",
    "# # Modify the DataFrame to keep rows where 'konsumgrp' is 45, 46, or 47\n",
    "# kpi_df = kpi_df[kpi_df['konsumgrp'].str[:2].isin([\"45\", \"46\", \"47\"])]\n",
    "\n",
    "# kpi_df = kpi_df.sort_values(by='konsumgrp')\n",
    "\n",
    "# kpi_df['value'] = kpi_df['value'] / 100\n",
    "\n",
    "# kpi_df.rename(columns={\"value\": \"inflation_rate\"}, inplace=True)\n",
    "\n",
    "# # pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# kpi_df['n4'] = kpi_df[konsumgrp]\n",
    "\n",
    "kpi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert string columns to numeric\n",
    "merged_df[\"gjeldende_bdr_syss\"] = pd.to_numeric(\n",
    "    merged_df[\"gjeldende_bdr_syss\"], errors=\"coerce\"\n",
    ")\n",
    "merged_df[\"fjor_syssel_t1\"] = pd.to_numeric(\n",
    "    merged_df[\"fjor_syssel_t1\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Perform division after conversion\n",
    "merged_df[\"emp_delta\"] = merged_df[\"gjeldende_bdr_syss\"] / merged_df[\"fjor_syssel_t1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "# Merge files with different features\n",
    "\n",
    "keys: b_kommunenr, fjor_omsetn_kr_t1\n",
    "\n",
    "inflation_rate_oms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputable_df = merged_df.copy()\n",
    "\n",
    "\n",
    "imputable_df = imputable_df.drop_duplicates(subset=[\"v_orgnr\"])\n",
    "\n",
    "# imputable_df['n4'] =  imputable_df['nacef_5'].str[:5]\n",
    "imputable_df[\"n4\"] = imputable_df[\"tmp_sn2007_5\"].str[:5]\n",
    "\n",
    "imputable_df = pd.merge(imputable_df, kommune_befolk, on=\"b_kommunenr\", how=\"left\")\n",
    "imputable_df = pd.merge(imputable_df, kommune_inn, on=\"b_kommunenr\", how=\"left\")\n",
    "imputable_df = pd.merge(imputable_df, kpi_df, on=\"n4\", how=\"left\")\n",
    "\n",
    "# Ensure columns are numeric\n",
    "imputable_df[\"fjor_omsetn_kr_t1\"] = pd.to_numeric(\n",
    "    imputable_df[\"fjor_omsetn_kr_t1\"], errors=\"coerce\"\n",
    ")\n",
    "imputable_df[\"inflation_rate\"] = pd.to_numeric(\n",
    "    imputable_df[\"inflation_rate\"], errors=\"coerce\"\n",
    ")\n",
    "imputable_df[\"befolkning_delta\"] = pd.to_numeric(\n",
    "    imputable_df[\"befolkning_delta\"], errors=\"coerce\"\n",
    ")\n",
    "imputable_df[\"emp_delta\"] = pd.to_numeric(imputable_df[\"emp_delta\"], errors=\"coerce\")\n",
    "imputable_df[\"inntekt_delta\"] = pd.to_numeric(\n",
    "    imputable_df[\"inntekt_delta\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "general_inflation_rate = imputable_df.loc[\n",
    "    imputable_df[\"n4\"] == \"47.78\", \"inflation_rate\"\n",
    "].values[0]\n",
    "imputable_df[\"inflation_rate\"] = imputable_df[\"inflation_rate\"].fillna(\n",
    "    general_inflation_rate\n",
    ")\n",
    "\n",
    "imputable_df[\"inflation_rate_oms\"] = (\n",
    "    imputable_df[\"fjor_omsetn_kr_t1\"] * imputable_df[\"inflation_rate\"]\n",
    ")\n",
    "imputable_df[\"befolkning_delta_oms\"] = (\n",
    "    imputable_df[\"fjor_omsetn_kr_t1\"] * imputable_df[\"befolkning_delta\"]\n",
    ")\n",
    "imputable_df[\"emp_delta_oms\"] = (\n",
    "    imputable_df[\"fjor_omsetn_kr_t1\"] * imputable_df[\"emp_delta\"]\n",
    ")\n",
    "imputable_df[\"inntekt_delta_oms\"] = (\n",
    "    imputable_df[\"fjor_omsetn_kr_t1\"] * imputable_df[\"inntekt_delta\"]\n",
    ")\n",
    "\n",
    "# imputable_df['inflation_rate_oms'] = imputable_df['inflation_rate_oms'].round(0).astype(int)\n",
    "# imputable_df['befolkning_delta_oms'] = imputable_df['befolkning_delta_oms'].round(0).astype(int)\n",
    "# imputable_df['emp_delta_oms'] = imputable_df['emp_delta_oms'].round(0).astype(int)\n",
    "# imputable_df['inntekt_delta_oms'] = imputable_df['inntekt_delta_oms'].round(0).astype(int)\n",
    "\n",
    "# Treat Nan for inflation_rate_oms\n",
    "imputable_df[\"inflation_rate_oms\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "group_means = imputable_df.groupby(\"nacef_5\")[\"inflation_rate_oms\"].transform(\"mean\")\n",
    "# Step 3: Fill NaN values in 'inflation_rate_oms' with the corresponding group's mean\n",
    "imputable_df[\"inflation_rate_oms\"].fillna(group_means, inplace=True)\n",
    "\n",
    "\n",
    "categories_to_impute = [\n",
    "    \"emp_delta_oms\",\n",
    "    \"befolkning_delta_oms\",\n",
    "    \"inntekt_delta_oms\",\n",
    "    \"inflation_rate_oms\",\n",
    "]\n",
    "\n",
    "# Identify rows where 'b_sysselsetting_syss' is equal to 0\n",
    "rows_to_impute = imputable_df[\"b_sysselsetting_syss\"] == 0\n",
    "\n",
    "# Replace NaN values with 0 for the identified rows and specified categories\n",
    "imputable_df.loc[rows_to_impute, categories_to_impute] = imputable_df.loc[\n",
    "    rows_to_impute, categories_to_impute\n",
    "].fillna(0)\n",
    "\n",
    "\n",
    "# Group by 'tmp_sn2007_5' and calculate the average 'emp_delta_oms'\n",
    "average_foretak_oms_pr_naring = imputable_df.groupby(\"tmp_sn2007_5\")[\n",
    "    \"foretak_omsetning\"\n",
    "].mean()\n",
    "\n",
    "# Create a new column 'average_emp_delt_oms_pr_naring' and assign the calculated averages to it\n",
    "imputable_df[\"average_emp_delt_oms_pr_naring\"] = imputable_df[\"nacef_5\"].map(\n",
    "    average_foretak_oms_pr_naring\n",
    ")\n",
    "imputable_df[\"average_emp_delt_oms_pr_naring\"] = (\n",
    "    imputable_df[\"average_emp_delt_oms_pr_naring\"].round(0).astype(int)\n",
    ")\n",
    "\n",
    "knn_df = imputable_df[\n",
    "    [\n",
    "        \"average_emp_delt_oms_pr_naring\",\n",
    "        \"emp_delta_oms\",\n",
    "        \"befolkning_delta_oms\",\n",
    "        \"inflation_rate_oms\",\n",
    "        \"inntekt_delta_oms\",\n",
    "        \"b_sysselsetting_syss\",\n",
    "        \"v_orgnr\",\n",
    "    ]\n",
    "]\n",
    "knn_df = knn_df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "# imputable_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a copy of your DataFrame\n",
    "imputable_df_copy = knn_df.copy()\n",
    "\n",
    "# Define the columns for numerical features\n",
    "numerical_features = [\n",
    "    \"average_emp_delt_oms_pr_naring\",\n",
    "    \"emp_delta_oms\",\n",
    "    \"befolkning_delta_oms\",\n",
    "    \"inflation_rate_oms\",\n",
    "    \"inntekt_delta_oms\",\n",
    "    \"b_sysselsetting_syss\",\n",
    "]\n",
    "\n",
    "numerical_features = [col for col in numerical_features if col != \"v_orgnr\"]\n",
    "\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[(\"num\", StandardScaler(), numerical_features)],\n",
    "    remainder=\"passthrough\",  # Keep non-numerical columns unchanged\n",
    ")\n",
    "\n",
    "knn_df[\"v_orgnr\"] = knn_df[\"v_orgnr\"].astype(str)\n",
    "\n",
    "# Create KNN imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Create imputer pipeline\n",
    "imputer_pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"imputer\", knn_imputer)])\n",
    "\n",
    "# Fit and transform the copy of your DataFrame\n",
    "imputed_values = imputer_pipeline.fit_transform(imputable_df_copy)\n",
    "\n",
    "\n",
    "# Convert the imputed values back to a DataFrame\n",
    "knn_df = pd.DataFrame(imputed_values, columns=imputable_df_copy.columns)\n",
    "\n",
    "\n",
    "# Update only the NaN values in the original DataFrame with the imputed values\n",
    "knn_df.update(knn_df, overwrite=False)\n",
    "\n",
    "# Inverse transform the scaled numerical features\n",
    "scaled_features = knn_df[numerical_features].values\n",
    "inverse_scaled_features = preprocessor.named_transformers_[\"num\"].inverse_transform(\n",
    "    scaled_features\n",
    ")\n",
    "knn_df[numerical_features] = inverse_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_df[\"v_orgnr\"] = knn_df[\"v_orgnr\"].round(0).astype(int)\n",
    "knn_df[\"v_orgnr\"] = knn_df[\"v_orgnr\"].astype(object)\n",
    "columns_to_drop = [\"average_emp_delt_oms_pr_naring\", \"b_sysselsetting_syss\"]\n",
    "knn_df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "knn_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_drop = [\n",
    "    \"emp_delta_oms\",\n",
    "    \"befolkning_delta_oms\",\n",
    "    \"inflation_rate_oms\",\n",
    "    \"inntekt_delta_oms\",\n",
    "]\n",
    "\n",
    "imputable_df.drop(columns=columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplicate_count_imputable = imputable_df[\"v_orgnr\"].duplicated().sum()\n",
    "print(\"Number of duplicates in imputable_df:\", duplicate_count_imputable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imputable_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert 'v_orgnr' column to strings\n",
    "knn_df[\"v_orgnr\"] = knn_df[\"v_orgnr\"].astype(str)\n",
    "imputable_df[\"v_orgnr\"] = imputable_df[\"v_orgnr\"].astype(str)\n",
    "\n",
    "# Strip 'v_orgnr' column in both knn_df and imputable_df\n",
    "knn_df[\"v_orgnr\"] = knn_df[\"v_orgnr\"].str.strip()\n",
    "imputable_df[\"v_orgnr\"] = imputable_df[\"v_orgnr\"].str.strip()\n",
    "\n",
    "imputable_df = pd.merge(imputable_df, knn_df, how=\"inner\", on=\"v_orgnr\")\n",
    "\n",
    "imputable_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "duplicate_v_orgnr_imputable = imputable_df[\"v_orgnr\"].duplicated().any()\n",
    "duplicate_v_orgnr_knn = knn_df[\"v_orgnr\"].duplicated().any()\n",
    "\n",
    "print(\"Duplicate v_orgnr in imputable_df:\", duplicate_v_orgnr_imputable)\n",
    "print(\"Duplicate v_orgnr in knn_df:\", duplicate_v_orgnr_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sample data with NaN values\n",
    "columns_for_imputation = [\n",
    "    \"new_oms\",\n",
    "    \"nacef_5\",\n",
    "    \"inntekt_delta_oms\",\n",
    "    \"emp_delta_oms\",\n",
    "    \"befolkning_delta_oms\",\n",
    "    \"inflation_rate_oms\",\n",
    "    \"v_orgnr\",\n",
    "    \"gjeldende_bdr_syss\",\n",
    "]\n",
    "\n",
    "#####################################\n",
    "# DIFFERENT FILTERING\n",
    "################################\n",
    "\n",
    "\n",
    "# Assuming 'reg_type' is the column in imputable_df\n",
    "# imputable_df_filtered = imputable_df[imputable_df['regtype'].isin(['01', '02', '03'])]\n",
    "imputable_df_filtered = imputable_df[~imputable_df[\"regtype\"].isin([\"04\", \"11\"])]\n",
    "\n",
    "\n",
    "#####################################\n",
    "# DIFFERENT FILTERING\n",
    "################################\n",
    "\n",
    "\n",
    "filtered_imputation_df = imputable_df_filtered[columns_for_imputation]\n",
    "filtered_imputation_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "\n",
    "# # Filter for rows with at least one non-NaN value in the specified columns\n",
    "# cleaned_imputation_df = filtered_imputation_df[filtered_imputation_df[['inntekt_delta_oms', 'emp_delta_oms', 'befolkning_delta_oms', 'inflation_rate_oms']].notna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select columns needed for the model (excluding 'new_oms')\n",
    "columns_for_imputation = filtered_imputation_df.columns.tolist()\n",
    "columns_for_imputation.remove(\"new_oms\")\n",
    "\n",
    "# Filter for rows where all columns (except 'new_oms') have no NaN values\n",
    "cleaned_imputation_df = filtered_imputation_df.dropna(\n",
    "    subset=columns_for_imputation, how=\"any\"\n",
    ")\n",
    "\n",
    "# Filter for rows where at least one column (excluding 'new_oms') has NaN values\n",
    "nn_df = filtered_imputation_df[\n",
    "    filtered_imputation_df[columns_for_imputation].isna().any(axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_imputation_df[\"inflation_rate_oms\"] = (\n",
    "    cleaned_imputation_df[\"inflation_rate_oms\"].round(0).astype(int)\n",
    ")\n",
    "cleaned_imputation_df[\"befolkning_delta_oms\"] = (\n",
    "    cleaned_imputation_df[\"befolkning_delta_oms\"].round(0).astype(int)\n",
    ")\n",
    "cleaned_imputation_df[\"emp_delta_oms\"] = (\n",
    "    cleaned_imputation_df[\"emp_delta_oms\"].round(0).astype(int)\n",
    ")\n",
    "cleaned_imputation_df[\"inntekt_delta_oms\"] = (\n",
    "    cleaned_imputation_df[\"inntekt_delta_oms\"].round(0).astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_count = cleaned_imputation_df[\"new_oms\"].isna().sum()\n",
    "print(f\"The number of NaN values in the 'new_oms' column is: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming you have cleaned_imputation_df and filtered_imputation_df\n",
    "\n",
    "# Get the indices of rows present in filtered_imputation_df\n",
    "filtered_indices = filtered_imputation_df.index.tolist()\n",
    "\n",
    "# Filter rows from filtered_imputation_df that are not present in cleaned_imputation_df (based on index)\n",
    "nn_df = filtered_imputation_df[\n",
    "    ~filtered_imputation_df.index.isin(cleaned_imputation_df.index)\n",
    "]\n",
    "\n",
    "# Alternative (less efficient for large DataFrames):\n",
    "# nn_df = filtered_imputation_df[~filtered_imputation_df.isin(cleaned_imputation_df).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_rows = nn_df[nn_df[\"emp_delta_oms\"].isna()]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(\"\\nRows where 'emp_delta_oms' is NaN:\")\n",
    "nan_rows.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cleaned_imputation_df.head()\n",
    "\n",
    "test_df = cleaned_imputation_df.copy()\n",
    "\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "test_df[\"real_oms\"] = test_df[\"new_oms\"]\n",
    "\n",
    "# Define the proportion of NaNs (adjust as needed)\n",
    "nan_proportion = 0.06  # 6% of values will be NaN - approx what we see in reality\n",
    "\n",
    "# Get the number of rows in the DataFrame\n",
    "num_rows = test_df.shape[0]\n",
    "\n",
    "# Calculate the number of rows to assign NaN\n",
    "num_nan = int(num_rows * nan_proportion)\n",
    "\n",
    "# Randomly sample indices for NaN assignment (without replacement)\n",
    "nan_indices = test_df.sample(\n",
    "    num_nan, random_state=42\n",
    ").index  # Set random_state for reproducibility\n",
    "\n",
    "test_df.loc[nan_indices, \"new_oms\"] = np.nan\n",
    "\n",
    "test_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, f_regression\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load your test data (assuming it's in a DataFrame named test_df)\n",
    "\n",
    "# # Separate features (excluding v_orgnr) and target variable\n",
    "# features = test_df.drop(['v_orgnr', 'real_oms', 'new_oms'], axis=1)\n",
    "# target = test_df['new_oms']\n",
    "\n",
    "# # Handle NaN values by dropping rows (consider alternative imputation if needed)\n",
    "# features_dropna = features.dropna()\n",
    "# target_dropna = target.loc[features_dropna.index]  # Align target with dropped rows\n",
    "\n",
    "# # Split data into training and testing sets (optional, for model evaluation)\n",
    "# X_train, X_test, y_train, y_test = train_test_split(features_dropna, target_dropna, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Feature selection with SelectKBest (f_regression for correlation-like measure)\n",
    "# selector = SelectKBest(f_regression, k=2)  # Select top 5 features\n",
    "# selector.fit(X_train, y_train)\n",
    "\n",
    "# # Print the selected features\n",
    "# print(\"Selected features:\")\n",
    "# print(features_dropna.columns[selector.get_support(indices=True)])\n",
    "\n",
    "# # Train a Random Forest model\n",
    "# model_rf = RandomForestRegressor()\n",
    "# model_rf.fit(X_train, y_train)\n",
    "\n",
    "# # Get feature importances from the Random Forest model\n",
    "# importances = model_rf.feature_importances_\n",
    "\n",
    "# # Print top 5 features based on Random Forest importance\n",
    "# print(\"\\nTop 5 features by Random Forest importance:\")\n",
    "# print(features_dropna.columns[importances.argsort()[-2:]])\n",
    "\n",
    "# # ... (Optional: Use the trained model for predictions on new data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interative Imputer + testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Works but no evaluation\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.metrics import mean_squared_error  # for evaluation (optional)\n",
    "from sklearn.metrics import mean_absolute_error  # for evaluation (optional)\n",
    "from sklearn.metrics import median_absolute_error  # for evaluation (optional)\n",
    "\n",
    "# Assuming cleaned_imputation_df is your existing dataframe\n",
    "\n",
    "# Make a copy to avoid modifying the original data\n",
    "X = cleaned_imputation_df.copy()\n",
    "\n",
    "# Separate features (excluding v_orgnr) and target variable\n",
    "features = X.drop([\"v_orgnr\", \"new_oms\"], axis=1)\n",
    "target = X[\"new_oms\"]\n",
    "\n",
    "# Impute missing values in features (if applicable)\n",
    "impute_it = IterativeImputer()\n",
    "impute_it.fit(features)\n",
    "features_transformed = impute_it.transform(features)\n",
    "features_df = pd.DataFrame(\n",
    "    features_transformed, columns=features.columns\n",
    ")  # Convert to DataFrame\n",
    "\n",
    "# Train your model here using features_df and target\n",
    "# (Replace this with your specific model training code)\n",
    "\n",
    "# Keep a copy of the original DataFrame for merging later\n",
    "original_df = X.copy()  # This maintains v_orgnr for merging\n",
    "\n",
    "# Create a DataFrame with imputed new_oms values (optional)\n",
    "imputed_df = features_df.copy()\n",
    "imputed_df[\"v_orgnr\"] = X[\"v_orgnr\"]  # Add v_orgnr from the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test imputation:\n",
    "\n",
    "# Make a copy to avoid modifying the original data\n",
    "X = test_df.copy()\n",
    "\n",
    "# Separate features (excluding v_orgnr) and target variable\n",
    "features = X.drop(\n",
    "    [\n",
    "        \"v_orgnr\",\n",
    "        \"real_oms\",\n",
    "        \"befolkning_delta_oms\",\n",
    "        \"inflation_rate_oms\",\n",
    "        \"emp_delta_oms\",\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "target = X[\"new_oms\"]\n",
    "\n",
    "# Impute missing values in features (if applicable)\n",
    "impute_it = IterativeImputer()\n",
    "impute_it.fit(features)\n",
    "features_transformed = impute_it.transform(features)\n",
    "features_df = pd.DataFrame(\n",
    "    features_transformed, columns=features.columns\n",
    ")  # Convert to DataFrame\n",
    "\n",
    "# Train your model here using features_df and target\n",
    "# (Replace this with your specific model training code)\n",
    "\n",
    "# Keep a copy of the original DataFrame for merging later\n",
    "original_test_df = X.copy()  # This maintains v_orgnr for merging\n",
    "\n",
    "# Create a DataFrame with imputed new_oms values (optional)\n",
    "imputed_test_df = features_df.copy()\n",
    "imputed_test_df[\"v_orgnr\"] = X[\"v_orgnr\"]  # Add v_orgnr from the original DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_df = pd.merge(\n",
    "    test_df, imputed_test_df[[\"v_orgnr\", \"new_oms\"]], how=\"left\", on=\"v_orgnr\"\n",
    ")\n",
    "evaluate_df = evaluate_df[\n",
    "    evaluate_df[\"new_oms_x\"].isna()\n",
    "]  # Filter rows where 'new_oms_x' is NaN\n",
    "\n",
    "evaluate_df.rename(columns={\"new_oms_y\": \"predicted_oms\"}, inplace=True)\n",
    "check = evaluate_df[evaluate_df[\"predicted_oms\"].isna()]\n",
    "evaluate_df = evaluate_df[evaluate_df[\"predicted_oms\"].notna()]\n",
    "\n",
    "evaluate_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_count = evaluate_df[\"predicted_oms\"].isna().sum()\n",
    "print(f\"The number of NaN values in the 'predicted_oms' column is: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_count = test_df[\"new_oms\"].isna().sum()\n",
    "print(f\"The number of NaN values in the 'new_oms' column is: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "check = evaluate_df[\n",
    "    evaluate_df[\"predicted_oms\"].isna()\n",
    "]  # Filter rows where 'new_oms_x' is NaN\n",
    "\n",
    "check[\"inntekt_delta_oms\"] = pd.to_numeric(\n",
    "    check[\"inntekt_delta_oms\"], errors=\"coerce\"\n",
    ")  # Attempt numeric conversion\n",
    "\n",
    "\n",
    "check.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "mse = mean_squared_error(evaluate_df[\"real_oms\"], evaluate_df[\"predicted_oms\"])\n",
    "mae = mean_absolute_error(evaluate_df[\"real_oms\"], evaluate_df[\"predicted_oms\"])\n",
    "medae = median_absolute_error(evaluate_df[\"real_oms\"], evaluate_df[\"predicted_oms\"])\n",
    "# (Optional) Percentage Error (PE) calculation\n",
    "# You might need to adjust the logic based on your data\n",
    "# evaluate_df['pe'] = (abs(evaluate_df['predicted_oms'] - evaluate_df['real_oms']) / evaluate_df['real_oms']) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "# (Optional) Print PE if calculated\n",
    "# print(f\"Percentage Error (PE): {evaluate_df['pe'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plot of predicted vs. real OMS\n",
    "plt.scatter(evaluate_df[\"real_oms\"], evaluate_df[\"predicted_oms\"])\n",
    "plt.xlabel(\"Real OMS\")\n",
    "plt.ylabel(\"Predicted OMS\")\n",
    "plt.title(\"Predicted vs. Real OMS\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Residual plot\n",
    "plt.scatter(\n",
    "    evaluate_df[\"real_oms\"], evaluate_df[\"predicted_oms\"] - evaluate_df[\"real_oms\"]\n",
    ")\n",
    "plt.xlabel(\"Real OMS\")\n",
    "plt.ylabel(\"Residuals (Predicted - Real)\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {
    "tags": []
   },
   "source": [
    "### With Every variable\n",
    "\n",
    "- Mean Squared Error (MSE): 510766522350.0012\n",
    "- Mean Absolute Error (MAE): 108769.08051029009\n",
    "- Median Absolute Error (MedAE): 24932.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {
    "tags": []
   },
   "source": [
    "## With Just employee delta\n",
    "\n",
    "- Mean Squared Error (MSE): 510144521616.9896\n",
    "- Mean Absolute Error (MAE): 108323.35968378294\n",
    "- Median Absolute Error (MedAE): 25709.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {
    "tags": []
   },
   "source": [
    "## With just inflation data\n",
    "\n",
    "- Mean Squared Error (MSE): 510672841283.873\n",
    "- Mean Absolute Error (MAE): 108804.11145815151\n",
    "- Median Absolute Error (MedAE): 25081.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# KNN Imputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make KNN test set and main dataset for imputing/fixing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_knn_df = test_df.copy()\n",
    "# # final_df = pd.merge(cleaned_imputation_df, df[['v_orgnr', 'new_oms']], how='left', on='v_orgnr')\n",
    "# test_knn_df = pd.merge(test_knn_df, imputable_df[['v_orgnr', 'nacef_5', 'b_kommunenr', 'b_sysselsetting_syss']], how='left', on='v_orgnr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hoved_df = pd.merge(\n",
    "    cleaned_imputation_df,\n",
    "    imputable_df[[\"v_orgnr\", \"tmp_sn2007_5\", \"b_kommunenr\", \"b_sysselsetting_syss\"]],\n",
    "    how=\"left\",\n",
    "    on=\"v_orgnr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hoved_df[\"b_sysselsetting_syss\"] = hoved_df[\"b_sysselsetting_syss\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count NaN values in each column\n",
    "nan_counts = hoved_df.isna().sum()\n",
    "\n",
    "# Filter columns with NaN values\n",
    "columns_with_nan = nan_counts[nan_counts > 0]\n",
    "\n",
    "# Print columns with NaN values and their respective counts\n",
    "print(\"Columns with NaN values:\")\n",
    "print(columns_with_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_count = hoved_df[\"new_oms\"].isna().sum()\n",
    "print(f\"The number of NaN values in the 'new_oms' column is: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hoved_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_knn_df['b_sysselsetting_syss'] = test_knn_df['b_sysselsetting_syss'].round(0).astype(int)\n",
    "# Convert the column to numeric, forcing errors to NaN\n",
    "hoved_df[\"b_sysselsetting_syss\"] = pd.to_numeric(\n",
    "    hoved_df[\"b_sysselsetting_syss\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Now you can round the values and convert them to integers\n",
    "hoved_df[\"b_sysselsetting_syss\"] = (\n",
    "    hoved_df[\"b_sysselsetting_syss\"].round(0).astype(\"Int64\")\n",
    ")  # Using 'Int64' to handle NaN values\n",
    "\n",
    "\n",
    "# knn_imputation_df = pd.merge(cleaned_imputation_df, imputable_df[['v_orgnr', 'nacef_5', 'b_kommunenr', 'b_sysselsetting_syss']], how='left', on='v_orgnr')\n",
    "# knn_imputation_df['b_sysselsetting_syss'] = knn_imputation_df['b_sysselsetting_syss'].round(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming cleaned_imputation_df is your existing dataframe\n",
    "\n",
    "# Because its imputing, you can include 'new_oms'\n",
    "\n",
    "# Make a copy to avoid modifying the original data\n",
    "\n",
    "# Use cleaned_imputation_df for real imputation\n",
    "# Use test_knn_df for test imputation\n",
    "\n",
    "# X = test_knn_df.copy()\n",
    "X = hoved_df.copy()\n",
    "\n",
    "drop_variables = [\"v_orgnr\", \"real_oms\"]\n",
    "\n",
    "# Separate features (excluding v_orgnr) and target variable\n",
    "features = X.drop([col for col in drop_variables if col in X.columns], axis=1)\n",
    "target = X[\"new_oms\"]\n",
    "\n",
    "# Define categorical features for target encoding\n",
    "categorical_features = [\"nacef_5\", \"b_kommunenr\"]\n",
    "\n",
    "# Create a dictionary to store average target (new_oms) for each category\n",
    "target_encoding_dict = {}\n",
    "for cat_feature in categorical_features:\n",
    "    target_encoding_dict[cat_feature] = features.groupby(cat_feature)[\"new_oms\"].mean()\n",
    "\n",
    "\n",
    "# Function to apply target encoding to a single feature\n",
    "def apply_target_encoding(df, feature, encoding_dict):\n",
    "    df[feature] = df[feature].replace(encoding_dict[feature].to_dict())\n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply target encoding to each categorical feature\n",
    "for cat_feature in categorical_features:\n",
    "    features = apply_target_encoding(features.copy(), cat_feature, target_encoding_dict)\n",
    "\n",
    "# # Handle missing values in numerical features (optional)\n",
    "# # You can replace this with your preferred imputation method\n",
    "# imputer = SimpleImputer(strategy='mean')\n",
    "# features = imputer.fit_transform(features)\n",
    "\n",
    "# Feature scaling (standardization in this example) on numerical features only\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# features_df = pd.DataFrame(scaled_features, columns=features.columns)  # Use original feature names\n",
    "\n",
    "# # Convert scaled features back to a DataFrame\n",
    "# scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "# ... (Rest of your code for processing other features)\n",
    "\n",
    "# Combine encoded categorical features with scaled numerical features\n",
    "# all_features = pd.concat([scaled_df, features[list(set(features.columns) - set(categorical_features))]], axis=1)\n",
    "\n",
    "\n",
    "# Impute missing values in features using KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "imputer.fit(scaled_features)\n",
    "features_transformed = imputer.transform(scaled_features)\n",
    "\n",
    "# Create a DataFrame with imputed features (including original column names)\n",
    "imputed_features = pd.DataFrame(features_transformed, columns=features.columns)\n",
    "\n",
    "# Inverse scaling on imputed features to get original scale back\n",
    "imputed_features = pd.DataFrame(\n",
    "    scaler.inverse_transform(imputed_features), columns=features.columns\n",
    ")\n",
    "\n",
    "# Train your model here using features_df and target\n",
    "# (Replace this with your specific model training code)\n",
    "\n",
    "# Combine imputed features with target variable (optional)\n",
    "imputed_knn_df = imputed_features.copy()\n",
    "# imputed_df['new_oms'] = target  # Assuming you want the target variable\n",
    "\n",
    "# Combine with organization number\n",
    "imputed_knn_df[\"v_orgnr\"] = X[\"v_orgnr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_count = imputed_knn_df[\"new_oms\"].isna().sum()\n",
    "print(f\"The number of NaN values in the 'new_oms' column is: {nan_count}\")\n",
    "\n",
    "original_nan_count = hoved_df[\"new_oms\"].isna().sum()\n",
    "print(\n",
    "    f\"The number of NaN values originally in the 'new_oms' column is: {original_nan_count}\"\n",
    ")\n",
    "\n",
    "shape = imputed_knn_df.shape\n",
    "shape_original = hoved_df.shape\n",
    "\n",
    "print(f\"The shape now is: {shape}\")\n",
    "print(f\"The shape originally was: {shape_original}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# evaluate_knn_df = pd.merge(test_knn_df, imputed_knn_df[['v_orgnr', 'new_oms']], how='left', on='v_orgnr')\n",
    "# evaluate_knn_df = evaluate_knn_df[evaluate_knn_df['new_oms_x'].isna()]  # Filter rows where 'new_oms_x' is NaN\n",
    "\n",
    "# evaluate_knn_df.rename(columns={\"new_oms_y\": \"predicted_oms\"}, inplace=True)\n",
    "# check = evaluate_knn_df[evaluate_knn_df['predicted_oms'].isna()]\n",
    "# evaluate_knn_df = evaluate_knn_df[evaluate_knn_df['predicted_oms'].notna()]\n",
    "\n",
    "# evaluate_knn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nan_count = evaluate_knn_df['predicted_oms'].isna().sum()\n",
    "# print(f\"The number of NaN values in the 'predicted_oms' column is: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nan_count = test_knn_df['new_oms'].isna().sum()\n",
    "# print(f\"The number of NaN values in the 'new_oms' column is: {nan_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Calculate metrics\n",
    "# mse = mean_squared_error(evaluate_knn_df['real_oms'], evaluate_knn_df['predicted_oms'])\n",
    "# mae = mean_absolute_error(evaluate_knn_df['real_oms'], evaluate_knn_df['predicted_oms'])\n",
    "# medae = median_absolute_error(evaluate_knn_df['real_oms'], evaluate_knn_df['predicted_oms'])\n",
    "# # (Optional) Percentage Error (PE) calculation\n",
    "# # You might need to adjust the logic based on your data\n",
    "# # evaluate_df['pe'] = (abs(evaluate_df['predicted_oms'] - evaluate_df['real_oms']) / evaluate_df['real_oms']) * 100\n",
    "\n",
    "# # Print the results\n",
    "# print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "# print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "# print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "# # (Optional) Print PE if calculated\n",
    "# # print(f\"Percentage Error (PE): {evaluate_df['pe'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Scatter plot of predicted vs. real OMS\n",
    "# plt.scatter(evaluate_knn_df['real_oms'], evaluate_knn_df['predicted_oms'])\n",
    "# plt.xlabel('Real Sales Turnover')\n",
    "# plt.ylabel('Predicted Sales Turnover')\n",
    "# plt.title('Predicted vs. Real Sales Turnover')\n",
    "# plt.show()\n",
    "\n",
    "# # Optional: Residual plot\n",
    "# plt.scatter(evaluate_knn_df['real_oms'], evaluate_knn_df['predicted_oms'] - evaluate_knn_df['real_oms'])\n",
    "# plt.xlabel('Real Sales Turnvoer')\n",
    "# plt.ylabel('Residuals (Predicted - Real)')\n",
    "# plt.title('Residual Plot')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Neural Networking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# New model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Check which columns contain NaN values\n",
    "# nan_columns = hoved_df.columns[hoved_df.isna().any()].tolist()\n",
    "\n",
    "# # Print the columns that contain NaN values\n",
    "# print(f\"Columns containing NaN values: {nan_columns}\")\n",
    "\n",
    "# nan_rows = hoved_df[hoved_df['tmp_sn2007_5'].isna() | hoved_df['b_kommunenr'].isna()]\n",
    "\n",
    "# nan_rows.head()\n",
    "\n",
    "# impute missing values:\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming hoved_df is your DataFrame\n",
    "\n",
    "# Replace any string 'NaN' with actual NaN for proper checking\n",
    "hoved_df.replace(\"nan\", np.nan, inplace=True)\n",
    "\n",
    "# Step 1: Delete the row with v_orgnr = 111111111\n",
    "hoved_df = hoved_df[hoved_df[\"v_orgnr\"] != \"111111111\"]\n",
    "\n",
    "# Step 2: Impute the missing values for the specified rows\n",
    "impute_values = {\n",
    "    \"921757131\": {\"tmp_sn2007_5\": \"46.710\", \"b_kommunenr\": \"0301\"},\n",
    "    \"998505461\": {\"tmp_sn2007_5\": \"32.990\", \"b_kommunenr\": \"4622\"},\n",
    "    \"928019691\": {\"tmp_sn2007_5\": \"32.990\", \"b_kommunenr\": \"4621\"},\n",
    "    \"928601196\": {\"tmp_sn2007_5\": \"62.020\", \"b_kommunenr\": \"1507\"},\n",
    "}\n",
    "\n",
    "for v_orgnr, values in impute_values.items():\n",
    "    if \"tmp_sn2007_5\" in values:\n",
    "        hoved_df.loc[hoved_df[\"v_orgnr\"] == v_orgnr, \"tmp_sn2007_5\"] = values[\n",
    "            \"tmp_sn2007_5\"\n",
    "        ]\n",
    "    if \"b_kommunenr\" in values:\n",
    "        hoved_df.loc[hoved_df[\"v_orgnr\"] == v_orgnr, \"b_kommunenr\"] = values[\n",
    "            \"b_kommunenr\"\n",
    "        ]\n",
    "\n",
    "# Ensure changes are applied and convert back to string\n",
    "hoved_df = hoved_df.astype({\"v_orgnr\": str, \"tmp_sn2007_5\": str, \"b_kommunenr\": str})\n",
    "\n",
    "# Verify the changes\n",
    "print(hoved_df[hoved_df[\"v_orgnr\"].isin(impute_values.keys())])\n",
    "\n",
    "# Check for any remaining NaN values in 'tmp_sn2007_5' or 'b_kommunenr'\n",
    "nan_rows = hoved_df[hoved_df[\"tmp_sn2007_5\"].isna() | hoved_df[\"b_kommunenr\"].isna()]\n",
    "print(nan_rows.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check for any remaining NaN values in 'tmp_sn2007_5' or 'b_kommunenr'\n",
    "nan_rows = hoved_df[hoved_df[\"tmp_sn2007_5\"].isna() | hoved_df[\"b_kommunenr\"].isna()]\n",
    "print(nan_rows.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nan_rows = hoved_df[hoved_df['tmp_sn2007_5'].isna() | hoved_df['b_kommunenr'].isna()]\n",
    "\n",
    "# nan_rows.head()\n",
    "\n",
    "# hoved_df.head()\n",
    "\n",
    "negative_oms_rows = hoved_df[hoved_df[\"new_oms\"] < 0]\n",
    "negative_oms_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    median_absolute_error,\n",
    "    r2_score,\n",
    ")\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming hoved_df is already defined and loaded\n",
    "\n",
    "# Separate features and target\n",
    "drop_variables = [\"v_orgnr\", \"nacef_5\"]\n",
    "\n",
    "# Filter the dataframe\n",
    "# small_hoved_df = hoved_df[hoved_df['new_oms'] < 100000]\n",
    "# large_hoved_df = hoved_df[hoved_df['new_oms'] > 100000]\n",
    "\n",
    "# Calculate the 80th percentile of 'new_oms' column\n",
    "top_percentile = hoved_df[\"new_oms\"].quantile(0.8)\n",
    "\n",
    "# Filter the dataframe\n",
    "# top_80_df = hoved_df[hoved_df['new_oms'] >= top_percentile]\n",
    "# bottom_80_df = hoved_df[hoved_df['new_oms'] < top_percentile]\n",
    "\n",
    "# All\n",
    "X = hoved_df.copy()\n",
    "\n",
    "# Small\n",
    "# X = small_hoved_df.copy()\n",
    "\n",
    "# # large\n",
    "# X = large_hoved_df.copy()\n",
    "\n",
    "# # Largest percentile\n",
    "# X = top_80_df.copy()\n",
    "\n",
    "# Smallest Percentile\n",
    "# X = bottom_80_df.copy()\n",
    "\n",
    "\n",
    "features = X.drop([col for col in drop_variables if col in X.columns], axis=1)\n",
    "target = X[\"new_oms\"]\n",
    "\n",
    "# Handle categorical features using target encoding\n",
    "categorical_features = [\"tmp_sn2007_5\", \"b_kommunenr\"]\n",
    "temp_df = features.copy()\n",
    "temp_df[\"new_oms\"] = target\n",
    "\n",
    "target_encoding_dict = {}\n",
    "for cat_feature in categorical_features:\n",
    "    target_encoding_dict[cat_feature] = temp_df.groupby(cat_feature)[\"new_oms\"].mean()\n",
    "\n",
    "\n",
    "def apply_target_encoding(df, feature, encoding_dict):\n",
    "    df[feature] = df[feature].replace(encoding_dict[feature].to_dict())\n",
    "    return df\n",
    "\n",
    "\n",
    "for cat_feature in categorical_features:\n",
    "    features = apply_target_encoding(features.copy(), cat_feature, target_encoding_dict)\n",
    "\n",
    "# Drop 'new_oms' from features\n",
    "features = features.drop(\"new_oms\", axis=1)\n",
    "\n",
    "# Split the data into non-NaN and NaN parts\n",
    "non_nan_mask = ~target.isna()\n",
    "features_non_nan = features[non_nan_mask]\n",
    "target_non_nan = target[non_nan_mask]\n",
    "features_nan = features[~non_nan_mask]\n",
    "\n",
    "# Feature scaling (standardization)\n",
    "scaler = StandardScaler()\n",
    "scaled_features_non_nan = scaler.fit_transform(features_non_nan)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_features_non_nan, target_non_nan, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=150,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "\n",
    "# Calculate mean standard error ratio\n",
    "mean_target = np.mean(y_test)\n",
    "mae_ratio = mae / mean_target\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "print(f\"Mean Standard Error Ratio: {mae_ratio}\")\n",
    "print(f\"R Squared: {r_squared}\")\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training History\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predicted vs. real OMS\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", lw=2)\n",
    "plt.xlabel(\"Real Sales Turnover\")\n",
    "plt.ylabel(\"Predicted Sales Turnover\")\n",
    "plt.title(\"Predicted vs. Real Sales Turnover\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Residual plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred - y_test, alpha=0.3)\n",
    "plt.hlines(0, y_test.min(), y_test.max(), colors=\"r\", linestyles=\"dashed\")\n",
    "plt.xlabel(\"Real Sales Turnover\")\n",
    "plt.ylabel(\"Residuals (Predicted - Real)\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n",
    "\n",
    "# Scale the features for NaN rows\n",
    "scaled_features_nan = scaler.transform(features_nan)\n",
    "\n",
    "# Predict missing 'new_oms' values\n",
    "predicted_nan = model.predict(scaled_features_nan).flatten()\n",
    "\n",
    "# Impute the predicted values back into the original dataframe\n",
    "X.loc[~non_nan_mask, \"new_oms\"] = predicted_nan\n",
    "\n",
    "# Verify the imputation\n",
    "print(X.head())\n",
    "print(X[~non_nan_mask].head())  # Check the rows where NaN values were imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93",
   "metadata": {
    "tags": []
   },
   "source": [
    "# XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hoved_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hoved_df = hoved_df.dropna()\n",
    "hoved_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# df = ml_df.copy()\n",
    "# df = pre_ml.copy()\n",
    "df = hoved_df.copy()\n",
    "# df = test_uten2017.copy()\n",
    "\n",
    "# df = df[np.isfinite(df[\"mengde\"])]\n",
    "# Replace infinite values with NaN\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "\n",
    "# Drop 'ident' column if not needed\n",
    "df = df.drop(columns=[\"v_orgnr\"])\n",
    "\n",
    "# Convert object types to category\n",
    "categorical_columns = [\"tmp_sn2007_5\", \"b_kommunenr\", \"nacef_5\"]\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].astype(\"category\")\n",
    "\n",
    "# Apply log transformation to `verdi`\n",
    "# df['verdi'] = np.log1p(df['verdi'])\n",
    "\n",
    "# Define features and target variable\n",
    "X = df.drop(columns=[\"new_oms\"])\n",
    "y = df[\"new_oms\"]\n",
    "\n",
    "# Define categorical and numerical features\n",
    "categorical_features = [\"tmp_sn2007_5\", \"b_kommunenr\", \"nacef_5\"]\n",
    "numerical_features = [\n",
    "    \"inntekt_delta_oms\",\n",
    "    \"emp_delta_oms\",\n",
    "    \"befolkning_delta_oms\",\n",
    "    \"inflation_rate_oms\",\n",
    "    \"gjeldende_bdr_syss\",\n",
    "]\n",
    "\n",
    "# # Preprocessing pipeline\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", \"passthrough\", numerical_features),  # No transformation for numerical features\n",
    "#         (\"cat\", OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\"), categorical_features),  # One-hot encoding for categorical features\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\n",
    "            \"num\",\n",
    "            StandardScaler(),\n",
    "            numerical_features,\n",
    "        ),  # Apply standard scaling to numerical features\n",
    "        (\n",
    "            \"cat\",\n",
    "            OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\"),\n",
    "            categorical_features,\n",
    "        ),  # One-hot encoding for categorical features\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Fit the preprocessor on the training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# # Transform the training and testing data\n",
    "# X_train_transformed = preprocessor.transform(X_train)\n",
    "# X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# Transform the training and testing data\n",
    "X_train_transformed = preprocessor.transform(X_train).toarray()\n",
    "X_test_transformed = preprocessor.transform(X_test).toarray()\n",
    "\n",
    "# Define the model\n",
    "# regressor = xgb.XGBRegressor(eval_metric='rmse', enable_categorical=True)\n",
    "regressor = xgb.XGBRegressor(eval_metric=\"mae\", enable_categorical=True)\n",
    "\n",
    "# Train the model with learning history\n",
    "eval_set = [(X_train_transformed, y_train), (X_test_transformed, y_test)]\n",
    "regressor.fit(\n",
    "    X_train_transformed,\n",
    "    y_train,\n",
    "    eval_set=eval_set,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = regressor.predict(X_test_transformed)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)  # Calculate Mean Absolute Error\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r_squared)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "\n",
    "# Cross-validation for more robust evaluation\n",
    "cv_scores = cross_val_score(\n",
    "    Pipeline([(\"preprocessor\", preprocessor), (\"regressor\", regressor)]),\n",
    "    X,\n",
    "    y,\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    ")\n",
    "print(\"Cross-Validation Mean Squared Error:\", -cv_scores.mean())\n",
    "\n",
    "# Plot the learning history\n",
    "results = regressor.evals_result()\n",
    "# epochs = len(results[\"validation_0\"][\"rmse\"])\n",
    "epochs = len(results[\"validation_0\"][\"mae\"])\n",
    "x_axis = range(0, epochs)\n",
    "plt.figure(figsize=(10, 5))\n",
    "# plt.plot(x_axis, results[\"validation_0\"][\"rmse\"], label=\"Train\")\n",
    "# plt.plot(x_axis, results[\"validation_1\"][\"rmse\"], label=\"Test\")\n",
    "plt.plot(x_axis, results[\"validation_0\"][\"mae\"], label=\"Train\")\n",
    "plt.plot(x_axis, results[\"validation_1\"][\"mae\"], label=\"Test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"XGBoost Learning History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Predicted vs. Actual Values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"r--\", lw=2)\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.title(\"Predicted vs. Actual Values\")\n",
    "plt.show()\n",
    "\n",
    "# Plot Residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.scatter(y_test, residuals, alpha=0.3)\n",
    "plt.hlines(0, y_test.min(), y_test.max(), colors=\"r\", linestyles=\"dashed\")\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals Plot\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Tree textual representation\n",
    "booster = regressor.get_booster()\n",
    "with open(\"dump.raw.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(booster.get_dump()))\n",
    "print(booster.get_dump()[0])  # Print the first tree\n",
    "\n",
    "# SHAP values\n",
    "explainer = shap.Explainer(regressor, X_train_transformed)\n",
    "shap_values = explainer(X_test_transformed)\n",
    "\n",
    "# Get feature names after one-hot encoding\n",
    "feature_names = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Summary plot of SHAP values\n",
    "shap.summary_plot(shap_values, X_test_transformed, feature_names=feature_names)\n",
    "\n",
    "# Force plot for a single prediction (e.g., the first instance)\n",
    "shap.initjs()\n",
    "shap.force_plot(\n",
    "    shap_values[0].base_values,\n",
    "    shap_values[0].values,\n",
    "    X_test_transformed[0],\n",
    "    feature_names=feature_names,\n",
    ")\n",
    "\n",
    "# Find the correct index for the feature \"verdi\"\n",
    "verdi_index = list(feature_names).index(\"num__inntekt_delta_oms\")\n",
    "\n",
    "# Dependence plot to show the effect of a single feature across the dataset\n",
    "shap.dependence_plot(\n",
    "    verdi_index, shap_values.values, X_test_transformed, feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Old model - works for dfs without NaN new_oms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    median_absolute_error,\n",
    ")\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming test_knn_df is your dataframe after merging imputable_df and test_df\n",
    "\n",
    "# Make a copy to avoid modifying the original data\n",
    "\n",
    "# Test data for evaluating = test_knn_df\n",
    "# real data = hoved_df\n",
    "\n",
    "# X = test_knn_df.copy()\n",
    "X = hoved_df.copy()\n",
    "\n",
    "drop_variables = [\"v_orgnr\", \"nacef_5\"]\n",
    "\n",
    "# Separate features (excluding v_orgnr) and target variable\n",
    "features = X.drop([col for col in drop_variables if col in X.columns], axis=1)\n",
    "target = X[\"new_oms\"]\n",
    "\n",
    "# Handle categorical features using target encoding\n",
    "categorical_features = [\"tmp_sn2007_5\", \"b_kommunenr\"]\n",
    "\n",
    "# Create a temporary dataframe including the target to compute the mean\n",
    "temp_df = features.copy()\n",
    "temp_df[\"new_oms\"] = target\n",
    "\n",
    "target_encoding_dict = {}\n",
    "for cat_feature in categorical_features:\n",
    "    target_encoding_dict[cat_feature] = temp_df.groupby(cat_feature)[\"new_oms\"].mean()\n",
    "\n",
    "\n",
    "def apply_target_encoding(df, feature, encoding_dict):\n",
    "    df[feature] = df[feature].replace(encoding_dict[feature].to_dict())\n",
    "    return df\n",
    "\n",
    "\n",
    "for cat_feature in categorical_features:\n",
    "    features = apply_target_encoding(features.copy(), cat_feature, target_encoding_dict)\n",
    "\n",
    "# Now drop 'new_oms' from features\n",
    "features = features.drop(\"new_oms\", axis=1)\n",
    "\n",
    "# Drop rows where target 'new_oms' is NaN\n",
    "non_nan_mask = ~target.isna()\n",
    "features_non_nan = features[non_nan_mask]\n",
    "target_non_nan = target[non_nan_mask]\n",
    "\n",
    "# Feature scaling (standardization in this example) on numerical features only\n",
    "scaler = StandardScaler()\n",
    "scaled_features_non_nan = scaler.fit_transform(features_non_nan)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_features_non_nan, target_non_nan, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Define a simple neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"linear\"))\n",
    "\n",
    "# Compile the model\n",
    "\n",
    "##mean absolute error 20403 with 100 epochs and 16 batch size\n",
    "model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "\n",
    "# mean absolute error 20507 with 100 epochs and 32 batch size\n",
    "# mean absolute error 20051 with 150 epochs and 16 batch size\n",
    "\n",
    "# model.compile(optimizer='rmsprop', loss='mean_absolute_error')\n",
    "\n",
    "# Train the model and measure time per epoch\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "# Train the model with modified parameters\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=150,\n",
    "    batch_size=8,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=300, batch_size=8, validation_split=0.2, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "# history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model.predict(X_test).flatten()\n",
    "\n",
    "# Calculate metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Median Absolute Error (MedAE): {medae}\")\n",
    "\n",
    "# Plot the training history\n",
    "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training History\")\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of predicted vs. real OMS\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Real Sales Turnover\")\n",
    "plt.ylabel(\"Predicted Sales Turnover\")\n",
    "plt.title(\"Predicted vs. Real Sales Turnover\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: Residual plot\n",
    "plt.scatter(y_test, y_pred - y_test)\n",
    "plt.xlabel(\"Real Sales Turnover\")\n",
    "plt.ylabel(\"Residuals (Predicted - Real)\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n",
    "\n",
    "# Get rows with NaN values in 'new_oms'\n",
    "nan_mask = target.isna()\n",
    "features_nan = features[nan_mask]\n",
    "\n",
    "# Scale the features\n",
    "scaled_features_nan = scaler.transform(features_nan)\n",
    "\n",
    "# Predict missing 'new_oms' values\n",
    "predicted_nan = model.predict(scaled_features_nan).flatten()\n",
    "\n",
    "x_resid = X.copy()\n",
    "\n",
    "# Impute the predicted values back into the original dataframe\n",
    "X.loc[nan_mask, \"new_oms\"] = predicted_nan\n",
    "\n",
    "# Verify the imputation\n",
    "print(X.head())\n",
    "print(X[nan_mask].head())  # Check the rows where NaN values were imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.head(100)\n",
    "\n",
    "nan_count = X[\"new_oms\"].isna().sum()\n",
    "print(f\"The number of NaN values in the 'new_oms' column is: {nan_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {
    "tags": []
   },
   "source": [
    "# find outliers - only for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Predict missing 'new_oms' values for the entire dataset\n",
    "scaled_features = scaler.transform(features)\n",
    "predicted_oms = model.predict(scaled_features).flatten()\n",
    "\n",
    "# Create a new DataFrame with original data and predicted values\n",
    "# x_resid = X.copy()\n",
    "x_resid[\"predicted_oms\"] = predicted_oms\n",
    "\n",
    "# Verify the new DataFrame\n",
    "x_resid.head()\n",
    "\n",
    "x_resid[\"residuals\"] = np.abs(x_resid[\"predicted_oms\"] - x_resid[\"real_oms\"])\n",
    "\n",
    "# x_resid['predicted_oms'] = x_resid['predicted_oms'].round(0).astype(int)\n",
    "# x_resid['residuals'] = x_resid['residuals'].round(0).astype(int)\n",
    "\n",
    "x_resid_sorted = x_resid.sort_values(by=\"residuals\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_resid_sorted_subset = x_resid_sorted.head(20)\n",
    "x_resid_sorted_subset[\"predicted_oms\"] = (\n",
    "    x_resid_sorted_subset[\"predicted_oms\"].round(0).astype(int)\n",
    ")\n",
    "x_resid_sorted_subset[\"residuals\"] = (\n",
    "    x_resid_sorted_subset[\"residuals\"].round(0).astype(int)\n",
    ")\n",
    "x_resid_sorted_subset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming 'common_column' is the column in both DataFrames to merge on\n",
    "merged_df_filtered = pd.merge(\n",
    "    merged_df, x_resid_sorted_subset, on=\"v_orgnr\", how=\"inner\"\n",
    ")\n",
    "\n",
    "merged_df_filtered[\"predicted_oms\"] = (\n",
    "    merged_df_filtered[\"predicted_oms\"].round(0).astype(int)\n",
    ")\n",
    "merged_df_filtered[\"residuals\"] = merged_df_filtered[\"residuals\"].round(0).astype(int)\n",
    "merged_df_filtered = merged_df_filtered.sort_values(by=\"residuals\", ascending=False)\n",
    "merged_df_filtered.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df_filtered.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training History Plot\n",
    "1. Training Loss and Validation Loss Decrease:\n",
    "\n",
    " - Both the training loss and validation loss show a decreasing trend over the epochs, indicating that the model is learning from the data.\n",
    " - The training loss starts higher than the validation loss and both decrease gradually, which is a good sign of learning without immediate overfitting.\n",
    "\n",
    "2. Gap Between Training and Validation Loss:\n",
    "\n",
    "- The validation loss is consistently lower than the training loss. This could suggest that regularization (e.g., dropout) is effectively preventing overfitting.\n",
    "- Ideally, the training loss should be slightly lower or close to the validation loss. A significant gap may indicate underfitting or an overly regularized model, but in this case, the gap isn't large enough to cause concern.\n",
    "\n",
    "# Predcited vs Real Sales Turnover Plot\n",
    "\n",
    "1. Cluster at Low Values:\n",
    "\n",
    "- There is a significant clustering of points at lower values of sales turnover, indicating many of your data points are in this range.\n",
    "- This is common if the majority of your data points have low sales turnover.\n",
    "\n",
    "2. Deviation at Higher Values:\n",
    "\n",
    "- The predictions start to deviate more from the true values as the real sales turnover increases. This indicates the model may be struggling to predict higher values accurately. (perhaps not a problem if we continue to collect breakdowns for higher turnover companies)\n",
    "- This kind of pattern is common in regression tasks with a wide range of target values. The model tends to be more accurate in the range where most training data points are located and less accurate in sparser regions.\n",
    "\n",
    "# Analysis of the Residual Plot\n",
    "\n",
    "1. Residuals Close to Zero for Lower Values:\n",
    "\n",
    "- A significant number of residuals are clustered around zero for lower values of sales turnover. This indicates that the model is performing well for these predictions, with errors being relatively small.\n",
    "\n",
    "2. Increased Variability at Higher Values:\n",
    "\n",
    "- As sales turnover increases, the residuals show more variability and are spread out more. This suggests that the model's performance decreases as the sales turnover value increases, resulting in larger prediction errors.\n",
    "\n",
    "3. Symmetry of Residuals:\n",
    "\n",
    "- The residuals appear to be fairly symmetrically distributed around zero. This indicates that the model does not have a systematic bias in its predictions (i.e., it does not consistently overpredict or underpredict across the entire range of values).\n",
    "\n",
    "4. Presence of Outliers:\n",
    "\n",
    "- There are some points far from the zero line, which can be considered outliers. These indicate instances where the model's predictions are significantly off from the actual values.\n",
    "- Outliers may indicate that there are some anomalies in the data or that the model struggles with certain patterns in the data.\n",
    "\n",
    "\n",
    "# Recommendations\n",
    "1. Data Imbalance:\n",
    "\n",
    "- If the data is heavily imbalanced (many low turnover values and few high turnover values), consider techniques like resampling, synthetic data generation (SMOTE), or applying different evaluation metrics that are robust to imbalances.\n",
    "\n",
    "2. Model Complexity:\n",
    "\n",
    "- Experiment with more complex models or ensemble methods (e.g., Random Forests, Gradient Boosting) to see if they handle higher values better.\n",
    "\n",
    "3. Feature Engineering:\n",
    "\n",
    "- Investigate additional feature engineering. Sometimes, additional or transformed features can help the model capture the underlying patterns better.\n",
    "\n",
    "4. Hyperparameter Tuning:\n",
    "\n",
    "- Perform hyperparameter tuning using techniques like grid search or random search to optimize the model parameters.\n",
    "\n",
    "5. Alternative Models:\n",
    "\n",
    "- Consider using alternative models or architectures that might be better suited for the specific characteristics of your data.\n",
    "\n",
    "6. Residual Analysis:\n",
    "\n",
    "- Analyze residuals to understand where the model is performing poorly. Look for patterns or systematic errors in the residuals to guide further improvements.\n",
    "\n",
    "7. Addressing High Variability at Higher Values:\n",
    "\n",
    "- Data Transformation: Apply a logarithmic transformation to the sales turnover values to reduce skewness and make the model less sensitive to high values.\n",
    "- Advanced Models: Try more complex models or ensemble methods like Random Forests, Gradient Boosting, or XGBoost, which might handle the variability better.\n",
    "- Feature Engineering: Introduce new features or polynomial features that might capture the underlying patterns more effectively.\n",
    "\n",
    "8. Handling Outliers:\n",
    "\n",
    "- Outlier Detection and Removal: Identify and remove or treat outliers before training the model. Techniques like IQR (Interquartile Range) or z-score can help in detecting outliers.\n",
    "- Robust Models: Use models that are robust to outliers, such as robust regression techniques.\n",
    "\n",
    "9. Model tuning\n",
    "\n",
    "- Regularization: Apply regularization techniques to avoid overfitting, which can help in better generalization to unseen data.\n",
    "\n",
    "10. Cross- Validation\n",
    "\n",
    "- Use cross-validation to ensure that the model's performance is consistent across different subsets of the data.\n",
    "\n",
    "# Conclusion\n",
    "- The model is learning and reducing the error over epochs, which is a positive sign. However, its performance in predicting higher sales turnover values indicates that there is room for improvement, possibly through more sophisticated techniques or additional data preprocessing.\n",
    "\n",
    "- The residual plot suggests that the model performs reasonably well for lower values of sales turnover but struggles with higher values, leading to higher prediction errors. Implementing the recommended improvements could help enhance the model's performance, especially in handling higher sales turnover values and reducing the prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='onlygoodoms'></a>\n",
    "# 1. Only Good Oms DF\n",
    "\n",
    "#### Uses the profit margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onlygoodoms[\"oms_share\"] = onlygoodoms[\"omsetn_kr\"] / onlygoodoms[\n",
    "    \"tot_oms_fordelt\"\n",
    "].round(5)\n",
    "\n",
    "# Round the values to whole numbers before assigning to the new columns\n",
    "onlygoodoms[\"new_oms\"] = (\n",
    "    (onlygoodoms[\"oms_share\"] * onlygoodoms[\"foretak_omsetning\"]).round(0).astype(int)\n",
    ")\n",
    "\n",
    "onlygoodoms[\"margin\"] = (\n",
    "    onlygoodoms[\"foretak_driftskostnad\"] / onlygoodoms[\"foretak_omsetning\"]\n",
    ")\n",
    "\n",
    "onlygoodoms[\"old_drkost\"] = onlygoodoms[\"driftskost_kr\"]\n",
    "\n",
    "onlygoodoms[\"new_drkost\"] = onlygoodoms[\"margin\"] * onlygoodoms[\"new_oms\"]\n",
    "\n",
    "onlygoodoms[\"driftskost_kr\"] = onlygoodoms[\"new_drkost\"]\n",
    "\n",
    "onlygoodoms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='gooddf'></a>\n",
    "# 4. Good DF AO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {
    "tags": []
   },
   "source": [
    "### rette oms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pd.set_option(\"display.max_rows\", None)\n",
    "\n",
    "\n",
    "good_df[\"oms_share\"] = good_df[\"omsetn_kr\"] / good_df[\"tot_oms_fordelt\"].round(5)\n",
    "\n",
    "# Round the values to whole numbers before assigning to the new columns\n",
    "good_df[\"new_oms\"] = (\n",
    "    (good_df[\"oms_share\"] * good_df[\"foretak_omsetning\"]).round(0).astype(int)\n",
    ")\n",
    "\n",
    "\n",
    "# good_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "### rette salgsint og forbruk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {
    "tags": []
   },
   "source": [
    "data salgsint_forbruk;\n",
    "\tset enhetene_brukes6  \n",
    "\t\t(\n",
    "\t\twhere=(rad_nr > 0)\n",
    "\t\tkeep=\n",
    "\t\torgnr_foretak \n",
    "\t\tlopenr\n",
    "\t\trad_nr\n",
    "\t\ttmp_ny_omsetn\n",
    "\t\tsalgsint\n",
    "\t\tforbruk\n",
    "\t\tnaring\n",
    "\t\tforetaksnaring\n",
    "\t\t\t);\n",
    "\tvhbed=0;\n",
    "\n",
    "\tif substr(naring,1,2) in &w_naring_vh. then\n",
    "\t\tvhbed=1;\n",
    "\n",
    "\tif naring in &w_nace1_ikke_vh. then\n",
    "\t\tvhbed=0;\n",
    "\n",
    "\tif substr(naring,1,4) in &w_nace2_ikke_vh. then\n",
    "\t\tvhbed=0;\n",
    "run;\n",
    "\n",
    "w_naring_vh = ('45', '46', '47')\n",
    "w_nace1_ikke_vh = 45.403\n",
    "w_nace2_ikke_vh =  ('45.2', '46.1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "# Define the values for w_naring_vh, w_nace1_ikke_vh, and w_nace2_ikke_vh\n",
    "w_naring_vh = (\"45\", \"46\", \"47\")\n",
    "w_nace1_ikke_vh = \"45.403\"\n",
    "w_nace2_ikke_vh = (\"45.2\", \"46.1\")\n",
    "\n",
    "enhetene_brukes = good_df.copy()\n",
    "\n",
    "# Filter the DataFrame based on conditions and create vhbed variable\n",
    "enhetene_brukes[\"vhbed\"] = 0\n",
    "\n",
    "# Check if the first two characters of 'naring' are in w_naring_vh\n",
    "enhetene_brukes.loc[\n",
    "    enhetene_brukes[\"tmp_sn2007_5\"].str[:2].isin(w_naring_vh), \"vhbed\"\n",
    "] = 1\n",
    "\n",
    "# Check if 'naring' is in w_nace1_ikke_vh\n",
    "enhetene_brukes.loc[enhetene_brukes[\"tmp_sn2007_5\"] == w_nace1_ikke_vh, \"vhbed\"] = 0\n",
    "\n",
    "# Check if the first four characters of 'naring' are in w_nace2_ikke_vh\n",
    "enhetene_brukes.loc[\n",
    "    enhetene_brukes[\"tmp_sn2007_5\"].str[:4].isin(w_nace2_ikke_vh), \"vhbed\"\n",
    "] = 0\n",
    "\n",
    "# enhetene_brukes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2\n",
    "salgsint_forbruk = enhetene_brukes[\n",
    "    [\n",
    "        \"orgnr_n_1\",\n",
    "        \"lopenr\",\n",
    "        \"v_orgnr\",\n",
    "        \"forbruk\",\n",
    "        \"salgsint\",\n",
    "        \"radnr\",\n",
    "        \"nacef_5\",\n",
    "        \"tmp_sn2007_5\",\n",
    "        \"new_oms\",\n",
    "        \"vhbed\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "# salgsint_forbruk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 3\n",
    "\n",
    "har = salgsint_forbruk[salgsint_forbruk.groupby(\"orgnr_n_1\")[\"vhbed\"].transform(\"any\")]\n",
    "# Extract the 'orgnr_n_1' column\n",
    "har = har[[\"orgnr_n_1\"]]\n",
    "\n",
    "# Remove duplicates\n",
    "har.drop_duplicates(inplace=True)\n",
    "\n",
    "# har.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 4\n",
    "\n",
    "ikke_har = salgsint_forbruk[\n",
    "    ~salgsint_forbruk.groupby(\"orgnr_n_1\")[\"vhbed\"].transform(\"any\")\n",
    "]\n",
    "ikke_har = ikke_har[[\"orgnr_n_1\"]]\n",
    "ikke_har.drop_duplicates(inplace=True)\n",
    "\n",
    "ikke_har[\"ikkevbed\"] = 1\n",
    "\n",
    "# ikke_har.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 5\n",
    "\n",
    "\n",
    "# Merge ikke_har into salgsint_forbruk with a left join on the 'id' column\n",
    "salgsint_forbruk_update1 = pd.merge(\n",
    "    salgsint_forbruk, ikke_har, on=\"orgnr_n_1\", how=\"left\"\n",
    ")\n",
    "\n",
    "# salgsint_forbruk_update1['ikkevbed'].fillna(0, inplace=True)\n",
    "\n",
    "# Update 'vhbed' to 1 where 'ikkevbed' is 1\n",
    "salgsint_forbruk_update1.loc[salgsint_forbruk_update1[\"ikkevbed\"] == 1, \"vhbed\"] = 1\n",
    "\n",
    "# salgsint_forbruk_update1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 6\n",
    "# Assuming your original DataFrame is named salgsint_forbruk_update1\n",
    "# Replace 'new_oms', 'orgnr_foretak', 'lopnr', 'vhbed' with the actual column names in your DataFrame\n",
    "\n",
    "# Create sum1 DataFrame for vhbed=1\n",
    "sum1 = (\n",
    "    salgsint_forbruk_update1[salgsint_forbruk_update1[\"vhbed\"] == 1]\n",
    "    .groupby([\"orgnr_n_1\", \"lopenr\"])[\"new_oms\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "sum1.rename(columns={\"new_oms\": \"sumoms_vh\"}, inplace=True)\n",
    "\n",
    "# Create sum2 DataFrame for vhbed=0\n",
    "sum2 = (\n",
    "    salgsint_forbruk_update1[salgsint_forbruk_update1[\"vhbed\"] == 0]\n",
    "    .groupby([\"orgnr_n_1\", \"lopenr\"])[\"new_oms\"]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "sum2.rename(columns={\"new_oms\": \"sumoms_andre\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform an outer join of sum1 and sum2 by 'orgnr_foretak' and 'lopnr'\n",
    "sum3 = pd.merge(sum1, sum2, on=[\"orgnr_n_1\", \"lopenr\"], how=\"outer\")\n",
    "# sum3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 8\n",
    "\n",
    "salgsint_forbruk_update2 = pd.merge(\n",
    "    salgsint_forbruk_update1, sum3, on=[\"orgnr_n_1\", \"lopenr\"], how=\"outer\"\n",
    ")\n",
    "# salgsint_forbruk_update2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 9 & 10\n",
    "# Assuming 'orgnr_n_1', 'lopenr', and 'rad_nr' are the actual column names\n",
    "# Replace them with the actual names in your DataFrame\n",
    "\n",
    "# Sort the DataFrame by 'orgnr_n_1', 'lopenr', and 'rad_nr'\n",
    "salgsint_forbruk_update2.sort_values(by=[\"orgnr_n_1\", \"lopenr\", \"radnr\"], inplace=True)\n",
    "\n",
    "salgsint_forbruk_update2.sort_values(by=[\"orgnr_n_1\", \"lopenr\", \"vhbed\"], inplace=True)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "# salgsint_forbruk_update2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 11\n",
    "\n",
    "# Assuming your DataFrame is named salgsint_forbruk\n",
    "# Replace 'orgnr_foretak', 'lopenr', 'vhbed', 'vhf' with the actual column names\n",
    "\n",
    "# Sort the DataFrame by 'orgnr_foretak' and 'lopenr'\n",
    "\n",
    "salgsint_forbruk_update3 = salgsint_forbruk_update2.copy()\n",
    "\n",
    "salgsint_forbruk_update3.sort_values(by=[\"orgnr_n_1\", \"lopenr\"], inplace=True)\n",
    "\n",
    "# Create a new variable 'vhf' based on the values of 'vhbed'\n",
    "salgsint_forbruk_update3[\"vhf\"] = salgsint_forbruk_update3.groupby(\n",
    "    [\"orgnr_n_1\", \"lopenr\"]\n",
    ")[\"vhbed\"].transform(\"first\")\n",
    "\n",
    "# Retain the value of 'vhf' from the first observation in each group\n",
    "salgsint_forbruk_update3[\"vhf\"] = salgsint_forbruk_update3.groupby(\n",
    "    [\"orgnr_n_1\", \"lopenr\"]\n",
    ")[\"vhf\"].transform(\"first\")\n",
    "\n",
    "# Apply labels to the variables\n",
    "salgsint_forbruk_update3[\"vhbed\"] = salgsint_forbruk_update3[\"vhbed\"].astype(str)\n",
    "salgsint_forbruk_update3[\"vhf\"] = salgsint_forbruk_update3[\"vhf\"].astype(str)\n",
    "\n",
    "label_map_vhbed = {\"1\": \"varehandelsbedrift\", \"0\": \"annen type bedrift\"}\n",
    "label_map_vhf = {\n",
    "    \"1\": \"foretaket har kun varehandelsbedrifter eller ingen\",\n",
    "    \"0\": \"har varehandel og annen bedrift (blandingsnÃ¦ringer)\",\n",
    "}\n",
    "\n",
    "salgsint_forbruk_update3[\"vhbed\"] = salgsint_forbruk_update3[\"vhbed\"].map(\n",
    "    label_map_vhbed\n",
    ")\n",
    "salgsint_forbruk_update3[\"vhf\"] = salgsint_forbruk_update3[\"vhf\"].map(label_map_vhf)\n",
    "\n",
    "# salgsint_forbruk_update3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 11\n",
    "\n",
    "# Assuming your DataFrame is named salgsint_forbruk\n",
    "# Replace 'vhf' with the actual name of your column\n",
    "\n",
    "# Filter rows where vhf is 'foretaket har kun varehandelsbedrifter eller ingen'\n",
    "vhf_condition = (\n",
    "    salgsint_forbruk_update3[\"vhf\"]\n",
    "    == \"foretaket har kun varehandelsbedrifter eller ingen\"\n",
    ")\n",
    "vhf_df = salgsint_forbruk_update3.loc[vhf_condition]\n",
    "\n",
    "# Filter rows where vhf is not 'foretaket har kun varehandelsbedrifter eller ingen'\n",
    "andre_df = salgsint_forbruk_update3.loc[~vhf_condition]\n",
    "\n",
    "# vhf_df.head()\n",
    "# andre_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 12\n",
    "\n",
    "vhf_df[\"nokkel\"] = vhf_df[\"new_oms\"] / vhf_df[\"sumoms_vh\"]\n",
    "\n",
    "# Convert 'salgsint' column to numeric\n",
    "vhf_df[\"salgsint\"] = pd.to_numeric(vhf_df[\"salgsint\"], errors=\"coerce\")\n",
    "vhf_df[\"forbruk\"] = pd.to_numeric(vhf_df[\"forbruk\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "vhf_df[\"bedr_salgsint\"] = round(vhf_df[\"salgsint\"] * vhf_df[\"nokkel\"])\n",
    "vhf_df[\"bedr_forbruk\"] = round(vhf_df[\"forbruk\"] * vhf_df[\"nokkel\"])\n",
    "\n",
    "# vhf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 13\n",
    "\n",
    "andre_df[\"forbruk\"] = pd.to_numeric(andre_df[\"forbruk\"], errors=\"coerce\")\n",
    "andre_df[\"salgsint\"] = pd.to_numeric(andre_df[\"salgsint\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "# Assuming 'andre' is your DataFrame\n",
    "andre_df[\"avanse\"] = andre_df[\"forbruk\"] / andre_df[\"salgsint\"]\n",
    "\n",
    "# Filter rows where vhbed is 1\n",
    "vh_bedriftene = andre_df[andre_df[\"vhbed\"] == \"varehandelsbedrift\"].copy()\n",
    "\n",
    "# Calculate 'nokkel', 'bedr_salgsint', and 'bedr_forbruk' for vh-bedriftene\n",
    "vh_bedriftene[\"nokkel\"] = vh_bedriftene[\"new_oms\"] / vh_bedriftene[\"sumoms_vh\"]\n",
    "vh_bedriftene[\"bedr_salgsint\"] = round(\n",
    "    vh_bedriftene[\"salgsint\"] * vh_bedriftene[\"nokkel\"]\n",
    ")\n",
    "vh_bedriftene.loc[\n",
    "    vh_bedriftene[\"bedr_salgsint\"] > vh_bedriftene[\"new_oms\"], \"bedr_salgsint\"\n",
    "] = vh_bedriftene[\"new_oms\"]\n",
    "vh_bedriftene[\"bedr_forbruk\"] = round(\n",
    "    vh_bedriftene[\"bedr_salgsint\"] * vh_bedriftene[\"avanse\"]\n",
    ")\n",
    "\n",
    "# Summarize vh-bedriftene\n",
    "brukt1 = (\n",
    "    vh_bedriftene.groupby([\"orgnr_n_1\", \"lopenr\"])\n",
    "    .agg({\"bedr_salgsint\": \"sum\", \"bedr_forbruk\": \"sum\"})\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Merge summarized values back to 'andre'\n",
    "andre = pd.merge(andre_df, brukt1, on=[\"orgnr_n_1\", \"lopenr\"], how=\"left\")\n",
    "\n",
    "# Calculate 'resten1' and 'resten2'\n",
    "andre[\"resten1\"] = andre[\"salgsint\"] - andre[\"bedr_salgsint\"]\n",
    "andre[\"resten2\"] = andre[\"forbruk\"] - andre[\"bedr_forbruk\"]\n",
    "\n",
    "# Filter rows where vhbed is not 1\n",
    "blanding_av_vh_og_andre = andre[andre[\"vhbed\"] != \"varehandelsbedrift\"].copy()\n",
    "\n",
    "# Calculate 'nokkel', 'bedr_salgsint', and 'bedr_forbruk' for blending of vh and other industries\n",
    "blanding_av_vh_og_andre[\"nokkel\"] = (\n",
    "    blanding_av_vh_og_andre[\"new_oms\"] / blanding_av_vh_og_andre[\"sumoms_andre\"]\n",
    ")\n",
    "blanding_av_vh_og_andre[\"bedr_salgsint\"] = round(\n",
    "    blanding_av_vh_og_andre[\"resten1\"] * blanding_av_vh_og_andre[\"nokkel\"]\n",
    ")\n",
    "blanding_av_vh_og_andre[\"bedr_forbruk\"] = round(\n",
    "    blanding_av_vh_og_andre[\"resten2\"] * blanding_av_vh_og_andre[\"nokkel\"]\n",
    ")\n",
    "\n",
    "# Combine the two subsets back into 'andre'\n",
    "andre = pd.concat([vh_bedriftene, blanding_av_vh_og_andre], ignore_index=True)\n",
    "\n",
    "andre.sort_values(by=[\"orgnr_n_1\", \"lopenr\"], inplace=True)\n",
    "\n",
    "oppdatere_hv = pd.concat([vhf_df, andre], ignore_index=True)\n",
    "\n",
    "oppdatere_hv = oppdatere_hv[\n",
    "    [\"orgnr_n_1\", \"lopenr\", \"radnr\", \"bedr_forbruk\", \"bedr_salgsint\"]\n",
    "]\n",
    "\n",
    "# oppdatere_hv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "enhetene_brukes2 = pd.merge(\n",
    "    enhetene_brukes, oppdatere_hv, on=[\"orgnr_n_1\", \"lopenr\", \"radnr\"]\n",
    ")\n",
    "\n",
    "# enhetene_brukes2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rettes = enhetene_brukes2.copy()\n",
    "\n",
    "rettes[\"oms\"] = rettes[\"new_oms\"]\n",
    "rettes[\"driftsk\"] = rettes[\"gjeldende_driftsk_kr\"]\n",
    "\n",
    "# Convert columns to numeric\n",
    "rettes[\"tot_driftskost_fordelt\"] = pd.to_numeric(\n",
    "    rettes[\"tot_driftskost_fordelt\"], errors=\"coerce\"\n",
    ")\n",
    "rettes[\"driftsk\"] = pd.to_numeric(rettes[\"driftsk\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "rettes[\"drkost_share\"] = rettes[\"driftsk\"] / rettes[\"tot_driftskost_fordelt\"]\n",
    "\n",
    "rettes[\"new_drkost\"] = rettes[\"drkost_share\"] * rettes[\"foretak_driftskostnad\"]\n",
    "\n",
    "# rettes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "# Kontrol Drkost for forbuk og lÃ¸nn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "rettes2 = rettes.copy()\n",
    "rettes2[\"drkost_temp\"] = rettes2[\"new_drkost\"]\n",
    "\n",
    "# Fill NaN in 'drkost_temp' with 0\n",
    "rettes2[\"drkost_temp\"] = rettes2[\"drkost_temp\"].fillna(0)\n",
    "\n",
    "rettes2[\"gjeldende_lonn_kr\"] = pd.to_numeric(\n",
    "    rettes2[\"gjeldende_lonn_kr\"], errors=\"coerce\"\n",
    ").fillna(0)\n",
    "rettes2[\"bedr_forbruk\"] = pd.to_numeric(\n",
    "    rettes2[\"bedr_forbruk\"], errors=\"coerce\"\n",
    ").fillna(0)\n",
    "\n",
    "\n",
    "rettes2[\"lonn_+_forbruk\"] = rettes2[\"gjeldende_lonn_kr\"] + rettes2[\"bedr_forbruk\"]\n",
    "\n",
    "# Perform the if operation\n",
    "condition = rettes2[\"drkost_temp\"] < rettes2[\"lonn_+_forbruk\"]\n",
    "rettes2[\"drkost_temp\"] = np.where(\n",
    "    condition, rettes2[\"lonn_+_forbruk\"], rettes2[\"drkost_temp\"]\n",
    ")\n",
    "rettes2[\"theif\"] = np.where(condition, 1, 0)\n",
    "\n",
    "# rettes2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "### Create df with a list of orgnr_foretak that have thieves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "dkvars = rettes2[rettes2.groupby(\"orgnr_n_1\")[\"theif\"].transform(\"any\")]\n",
    "\n",
    "# dkvars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate 'utskudd'\n",
    "dkvars[\"utskudd\"] = (\n",
    "    dkvars[\"new_drkost\"] - dkvars[\"gjeldende_lonn_kr\"] - dkvars[\"bedr_forbruk\"]\n",
    ")\n",
    "dkvars[\"utskudd\"] = abs(dkvars[\"utskudd\"])\n",
    "\n",
    "# Keep selected columns\n",
    "columns_to_keep = [\n",
    "    \"orgnr_n_1\",\n",
    "    \"lopenr\",\n",
    "    \"radnr\",\n",
    "    \"utskudd\",\n",
    "    \"new_drkost\",\n",
    "    \"drkost_temp\",\n",
    "    \"theif\",\n",
    "    \"gjeldende_lonn_kr\",\n",
    "    \"bedr_forbruk\",\n",
    "]\n",
    "dkvars = dkvars[columns_to_keep]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# dkvars.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate sum of 'utskudd' grouped by 'orgnr_foretak', 'lopenr', and 'tyv'\n",
    "sum7b = dkvars.groupby([\"orgnr_n_1\", \"lopenr\", \"theif\"])[\"utskudd\"].sum().reset_index()\n",
    "\n",
    "# Display the result\n",
    "# sum7b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transpose the result\n",
    "sum7b_transposed = sum7b.pivot(\n",
    "    index=[\"orgnr_n_1\", \"lopenr\"], columns=\"theif\", values=\"utskudd\"\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns as per SAS code\n",
    "sum7b_transposed.rename(columns={0: \"thief0\", 1: \"thief1\"}, inplace=True)\n",
    "\n",
    "sum7b_transposed = sum7b_transposed[[\"orgnr_n_1\", \"lopenr\", \"thief0\", \"thief1\"]]\n",
    "\n",
    "# Display the transposed result\n",
    "# sum7b_transposed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merge sums\n",
    "dkvars_2 = pd.merge(dkvars, sum7b_transposed, on=[\"orgnr_n_1\", \"lopenr\"], how=\"inner\")\n",
    "# dkvars_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Apply conditional logic\n",
    "pd.set_option(\"display.float_format\", \"{:.2f}\".format)\n",
    "dkvars_2[\"andel1\"] = np.where(\n",
    "    dkvars_2[\"theif\"] == 0, dkvars_2[\"utskudd\"] / dkvars_2[\"thief0\"], np.nan\n",
    ")\n",
    "dkvars_2[\"andel2\"] = np.where(\n",
    "    dkvars_2[\"theif\"] == 0, np.round(dkvars_2[\"andel1\"] * dkvars_2[\"thief1\"]), np.nan\n",
    ")\n",
    "# dkvars_2['new_drkost'] = np.where(dkvars_2['theif'] == 0, np.sum(dkvars_2['drkost_temp'] - dkvars_2['andel2'], axis=0), dkvars_2['drkost_temp'])\n",
    "dkvars_2[\"new_drkost\"] = np.where(\n",
    "    dkvars_2[\"theif\"] == 0,\n",
    "    dkvars_2[\"drkost_temp\"] - dkvars_2[\"andel2\"],\n",
    "    dkvars_2[\"drkost_temp\"],\n",
    ")\n",
    "\n",
    "# Keep selected columns\n",
    "columns_to_keep = [\"orgnr_n_1\", \"lopenr\", \"radnr\", \"new_drkost\"]\n",
    "dkvars_3 = dkvars_2[columns_to_keep]\n",
    "\n",
    "# dkvars_2.head(50)\n",
    "\n",
    "good_final = dkvars_2.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {
    "tags": []
   },
   "source": [
    "Steps:\n",
    "\n",
    "1. Define the varehandel nÃ¦ringer that need salgsint and forbruk. \n",
    "2. Create a df called salgsint_forbruk that has the forbruk of the foretak, the salgsint of the foretak, orgnr_foretak, rad_nr, naring, foretaksnaring, the new oms and a column called vhbed which indicates 1 if it is a varehandel bedrift and 0 if it is not. \n",
    "3. create another df called har that is simply a list of orgnr_foretak where there is at least one bedrift that is in varehandel. Remove duplicates. \n",
    "4. create another df called ikke_har that is a list of orgnr_foretak that has 0 bedrifter in varehandel. Create a new variable called ikkevbed = 1. Remove duplicates. \n",
    "5. Update salgint_forbruk by merging salgsint_forbruk and ikke with a left join. If ikkevhbed=1 then vhbed=1. Basically what is happenign is that if a foretak has no varehandel, then salgsint will be distributed without bias to varehandel bedrifter. \n",
    "6. Create two new dfs. One is called sum1. This sums up all new_oms grouped by orgnr_foretak and lopnr for only if vbed=1. This new variable will be called sumoms_vh. The second df will be called sum2 and will be for only where vhbed=0. This new sum will be a variable called sumoms_andre. \n",
    "7. create a new df called sum3 that merges sum1 and sum2 by orgnr_foretak and lopnr. Outerjoin. \n",
    "8. update salgsint_forbruk by performing an outerjoin with sum3 by orgnr_foretak and lopnr. Outerjoin. \n",
    "9. Sort by orgnr_foretak, lopenr and rad_nr.\n",
    "10. VERY IMPORTANt. SORT BY ORGNR_FORETAK , LOPENR and VHBED. \n",
    "11. data salgsint_forbruk;\n",
    "\tset salgsint_forbruk;\n",
    "\tby orgnr_foretak lopenr;\n",
    "\n",
    "\tif first.orgnr_foretak and first.lopenr then\n",
    "\t\tvhf=vhbed;\n",
    "\tretain vhf;\n",
    "\tlabel\n",
    "\t\tvhbed ='1=varehandelsbedrift, 0 er annen type bedrift'\n",
    "\t\tvhf   ='1=foretaket har kun varehandelsbedrifter eller ingen , 0=har varehandel og annen bedrift (blandingsnÃ¦ringer)'\n",
    "\t;\n",
    "run;\n",
    "\n",
    "proc sort data=salgsint_forbruk;\n",
    "\tby orgnr_foretak lopenr rad_nr;\n",
    "run;\n",
    "\n",
    "*(6) deler opp i rene varehandelsforetak og i andre fioretak;\n",
    "data vhf  andre;\n",
    "\tset salgsint_forbruk;\n",
    "\n",
    "\tif vhf=1 then\n",
    "\t\toutput vhf;\n",
    "\telse output andre;\n",
    "run;\n",
    "\n",
    "Basically what is happening here, is that we are creating two tables. One that has foretak that ONLY have varehandel bedrift OR foretak that have 0 varehandle bedrifter. \n",
    "\n",
    "The two tables will be called vhf and andre. \n",
    "\n",
    "12. for vhf distribute salgsint and forbruk :\n",
    "\n",
    "data vhf;\n",
    "\tset vhf;\n",
    "\tnokkel = tmp_ny_omsetn / sumoms_vh;\n",
    "\tbedr_salgsint = round(salgsint * nokkel);\n",
    "\tbedr_forbruk  = round(forbruk * nokkel);\n",
    "run;\n",
    "\n",
    "13. for andre. \n",
    "\n",
    "data andre;\n",
    "\tset andre;\n",
    "\tavanse=forbruk / salgsint;\n",
    "\n",
    "\t*vh-bedriftene;\n",
    "\tif vhbed=1 then\n",
    "\t\tdo;\n",
    "\t\t\tnokkel = tmp_ny_omsetn / sumoms_vh;\n",
    "\t\t\tbedr_salgsint = round(salgsint * nokkel);\n",
    "\n",
    "\t\t\tif bedr_salgsint > tmp_ny_omsetn then\n",
    "\t\t\t\tbedr_salgsint = tmp_ny_omsetn;\n",
    "\t\t\tbedr_forbruk  = round(bedr_salgsint * avanse);\n",
    "\t\tend;\n",
    "run;\n",
    "\n",
    "*(8b) summerere vh-bedriftene;\n",
    "proc sql stimer;\n",
    "\tcreate table brukt1 \n",
    "\t\tas select orgnr_foretak, lopenr, sum(bedr_salgsint) as brukt1, sum(bedr_forbruk) as brukt2\n",
    "\t\t\tfrom andre \n",
    "\t\t\t\twhere vhbed=1\n",
    "\t\t\t\t\tgroup by orgnr_foretak, lopenr\n",
    "\t\t\t\t\t\torder by orgnr_foretak, lopenr\n",
    "\t;\n",
    "\tcreate table andre  \n",
    "\t\tas select a.*, b.*\n",
    "\t\t\tfrom andre  a, brukt1  b\n",
    "\t\t\t\twhere a.orgnr_foretak = b.orgnr_foretak and a.lopenr = b.lopenr\n",
    "\t\t\t\t\torder by a.orgnr_foretak, a.lopenr, a.rad_nr\n",
    "\t;\n",
    "quit;\n",
    "\n",
    "*(8c) sluttfordeler for de foretakene som har blanding av vh og andre nÃ¦ringer;\n",
    "data andre;\n",
    "\tset andre;\n",
    "\tresten1 = sum(salgsint, -brukt1);\n",
    "\tresten2 = sum(forbruk,-brukt2);\n",
    "\n",
    "\tif vhbed ne 1 then\n",
    "\t\tdo;\n",
    "\t\t\tnokkel = tmp_ny_omsetn / sumoms_andre;\n",
    "\t\t\tbedr_salgsint = round(resten1 * nokkel);\n",
    "\t\t\tbedr_forbruk  = round(resten2 * nokkel);\n",
    "\t\tend;\n",
    "run;\n",
    "\n",
    "# What is left to do:\n",
    "\n",
    "### Correct salgsint and forbruk for good_df. (instructions in cell above) Look at the program in sas as well to know how to do it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138",
   "metadata": {},
   "source": [
    "### Correct drkost for good_df. Have to make sure it passes with salary and forbruk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "### Do the same for the mixed_dfs. (good oms, bad costs etc etc ) Will need to selectivly treat based on what is good and what isnt. Maybe its worth just fixing bad oms. Bad costs can be fixed based on oms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "### Implement a the ml models for the bad_df. Maybe make it so you can choose which one to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
